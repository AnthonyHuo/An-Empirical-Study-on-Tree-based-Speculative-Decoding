[2024-01-31 13:52:45,619] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Namespace(model='JackFram/llama-68m', target='meta-llama/Llama-2-7b-hf', dataset='dataset/c4_small.json', start=0, end=200, T=0.6, P=1.0, DP=0.99, ALG='coverplus', D=1, B=10, W=32, M=384, Mode='benchmark', decay=0.85, negative=False, static=False, offloading=False)
1.9692307692307693
1.9104477611940298
1.8987341772151898
1.8982035928143712
1.8898305084745763
1.888157894736842
1.9
1.898477157360406
1.898477157360406
1.9065217391304348
1.8962264150943395
1.901006711409396
1.901006711409396
1.9042207792207793
[2024-01-31 13:54:10,758] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Namespace(model='JackFram/llama-68m', target='meta-llama/Llama-2-7b-hf', dataset='dataset/c4_small.json', start=0, end=200, T=0.6, P=1.0, DP=0.99, ALG='coverplus', D=1, B=10, W=32, M=384, Mode='benchmark', decay=0.85, negative=False, static=False, offloading=False)
1.8970588235294117
1.9179104477611941
1.905759162303665
1.905
1.8921933085501859
1.8988095238095237
1.8985148514851484
1.900212314225053
1.900212314225053
1.8998144712430427
1.8963815789473684
1.9034175334323922
1.9034175334323922
1.902834008097166
1.9070631970260223
1.9073226544622426
1.9136460554371002
1.9163346613545817
1.9169000933706817
1.9182058047493404
1.9170124481327802
1.9170124481327802
1.920472440944882
1.9214071856287425
1.9222539229671898
1.9237057220708447
1.9225260416666667
1.925625
1.9244604316546763
1.924154025670945
1.9258010118043845
1.9258028792912514
1.9258028792912514
1.9278460716194548
1.9272445820433437
1.9295704295704297
1.9295704295704297
1.930817610062893
1.930178069353327
1.930909090909091
1.9315975286849074
1.9318181818181819
1.932026688907423
1.932224025974026
1.9339398734177216
1.9339398734177216
1.9325885978428352
1.9342105263157894
1.9343360234776228
1.9330708661417322
1.9315403422982884
1.9317173096620006
1.9315754339118825
1.931538717181788
1.9320325203252033
1.9320325203252033
1.9297297297297298
1.930239898989899
1.9300787401574804
1.9296730413325107
1.930752948291503
1.9314946619217082
1.9322083212103578
1.9328954882924043
1.9327542729055758
1.9327542729055758
1.9336818932306
1.9337837837837837
1.9336927223719678
1.9335451416468097
1.9336455893832942
1.9339984650805833
1.9338531187122736
1.93490099009901
1.934973209936678
1.9348190750059908
1.935803634647156
1.9358587032303045
1.9359537572254335
1.935792349726776
1.9360699865410498
1.9365465399071413
1.936587491828285
1.936587491828285
1.936842105263158
1.9358594411515664
1.9358594411515664
1.9361169102296452
1.9361169102296452
1.9359687049619105
1.9348355663824603
1.9352835103185735
1.935337156416848
1.9359625146427177
1.9354527938342967
1.935471480699059
1.9338764683592269
1.9341317365269461
1.9325946445060018
1.9326765188834154
1.932592457642558
1.9326471650415313
1.9326004685528924
1.9328584149599288
1.931234611326064
1.931328233657858
1.9300927516317417
1.930366847826087
1.930366847826087
1.929818670248489
1.9294488711819389
1.930190538764783
1.9302892427689309
1.9308348077851054
1.9309247174916442
1.9313169502205418
1.9304971170328815
1.9304971170328815
1.9301464919043947
1.9302396580674706
1.930482091582288
1.9301869722557299
1.9304269931322784
1.9309478042288926
1.9306104523495828
1.9308394954327968
1.9309205802096798
1.9307254623044097
1.9306749330703115
1.9306749330703115
1.9306253489670575
1.9307053941908714
1.9310486634681288
1.930945246526832
1.9311555075593954
1.931745182012848
1.9311762365733987
1.9312071692145494
1.9310254735467014
1.931476683937824
1.931083527282131
1.931234679396207
1.9314314954586158
1.9318700837350926
1.931689520694427
1.931994010481657
1.9321782178217821
1.9322640345465762
1.9322640345465762
1.9322075379344101
1.9322651128914785
1.9322651128914785
1.932442196531792
1.9317883168080277
1.9321953532479847
1.9320239915324002
1.932422969187675
1.9324762566597173
1.9324762566597173
1.9324215607401447
1.932808578599133
1.932834127524393
1.9328828828828828
1.9327721661054995
1.9330362116991644
1.9335103440646089
1.933421226104153
1.9334358523725834
1.9336896062820372
1.9340480831708902
1.9337776822188777
1.9336844368013757
1.9335473515248796
1.9339001062699257
1.933920237590157
1.9341620141156641
1.9341620141156641
1.9336958795231123
1.934136713068772
1.9335740072202166
1.93354593148853
1.933463595228301
1.933470437017995
1.9335103666632623
1.9335103666632623
1.9336512123364107
1.9339850836524894
1.934214478822469
1.9341555599761289
1.9342718350351937
1.9341146346267482
1.9341146346267482
1.934435854780311
1.934204113461351
1.934204113461351
1.9343434343434343
1.934658816716533
1.9346887887215882
1.934718383684361
1.934792955735364
total decoding steps: 20325 large model steps: 10505 avg decoding step: 1.934792955735364
tensor([0.6316, 0.1000, 0.0485, 0.0292, 0.0226, 0.0150, 0.0107, 0.0090, 0.0070,
        0.0067, 0.0057, 0.0063, 0.0043, 0.0034, 0.0028, 0.0035, 0.0027, 0.0030,
        0.0022, 0.0021, 0.0011, 0.0010, 0.0016, 0.0021, 0.0017, 0.0010, 0.0013,
        0.0015, 0.0019, 0.0016, 0.0014, 0.0022, 0.0651], device='cuda:0')
tensor([0.6316, 0.7316, 0.7801, 0.8093, 0.8319, 0.8469, 0.8576, 0.8666, 0.8737,
        0.8803, 0.8861, 0.8923, 0.8966, 0.9000, 0.9028, 0.9063, 0.9090, 0.9120,
        0.9142, 0.9163, 0.9175, 0.9184, 0.9200, 0.9221, 0.9238, 0.9249, 0.9262,
        0.9277, 0.9297, 0.9313, 0.9327, 0.9349, 1.0000], device='cuda:0')
