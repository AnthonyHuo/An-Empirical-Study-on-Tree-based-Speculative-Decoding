{
    "sourceFile": "GreedySTree.py",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 1,
            "patches": [
                {
                    "date": 1712785368635,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                },
                {
                    "date": 1712785448029,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -57,9 +57,8 @@\n         tree_mask.masked_fill_(tree_mask > 0, torch.finfo(self.dtype).min)\n         self.initialize(attn_mask, sequence, new_tokens_buffer, parents_buffer, position_ids, None)\n         self.set_prefix(prefix=prefix)\n         self.tree_size = self.grow_map[\"size\"]\n-        print(self.tree_size)\n         self.tree_mask = tree_mask\n \n         self.full_attn_mask[self.max_length - self.tree_size + 1: self.max_length, self.max_length - self.tree_size + 1: self.max_length] = tree_mask[1:, 1:]\n \n"
                }
            ],
            "date": 1712785368635,
            "name": "Commit-0",
            "content": "import torch\nfrom transformers import AutoModelForCausalLM\nfrom transformers import LlamaForCausalLM, LlamaTokenizer, DataCollatorForLanguageModeling, OPTForCausalLM, AutoTokenizer\nfrom Llama import LlamaModel_Attn, LlamaForCausalLM_Attn\nfrom torch.nn.functional import softmax\nfrom copy import deepcopy\nfrom tqdm import tqdm\nfrom Tree import Tree\nimport time\nimport deepspeed\nfrom Engine import GraphInferenceEngine, GraphInferenceEngineTG\nfrom torch.profiler import profile, record_function, ProfilerActivity\nfrom utils import get_sampling_logits, make_tree_attention_mask, select_kv, ChildrenAccept, get_residual, cat_kv, _make_causal_mask\nclass GreedySTree(Tree):\n    def __init__(self, \n                 #draft_model :LlamaForCausalLM_Attn, \n                 draft_model_engine :GraphInferenceEngine,\n                 target_model_engine :GraphInferenceEngineTG,\n                 prefix :torch.LongTensor,\n                 temperature :float = 0.6,\n                 top_p: float = 0.9,\n                 draft_kv_len = 0,\n                 target_kv_len = 0,\n                 max_length = 256,\n                 device :str = 'cpu',\n                 max_target_seq = 256,\n                 vocab_size = 32000,\n                 grow_map = None,\n                 attn_mask = None, \n                 sequence = None, \n                 new_tokens_buffer = None, \n                 parents_buffer = None, \n                 position_ids = None,\n                 residual_graph = None,\n                 sampling_callables = None,\n                 sample_gather_indices = None) -> None:\n        super().__init__(device=device, max_length=max_length)\n        assert self.max_length == draft_model_engine.engine.max_length\n        self.max_target_seq = max_target_seq\n        #self.draft_model = draft_model.to(self.device).eval()\n        self.draft_model_engine = draft_model_engine\n        self.target_model_engine = target_model_engine\n        self.temperature = temperature\n        self.top_p = top_p\n        self.residual_graph = residual_graph\n        self.grow_map = grow_map\n        self.sampling_callables = sampling_callables\n        self.sample_gather_indices = sample_gather_indices\n        self.draft_step = len(self.grow_map[\"roots\"])\n        self.grow_map_roots_gpu = []\n        for x in self.grow_map[\"roots\"]:\n             self.grow_map_roots_gpu.append(torch.Tensor(x).to(self.device).long())\n        self.Successors = self.grow_map[\"Successors\"]\n        tree_mask :torch.Tensor = self.grow_map[\"mask\"].to(self.device)\n        tree_mask = (tree_mask == 0).type(self.dtype)\n        \n        tree_mask.masked_fill_(tree_mask > 0, torch.finfo(self.dtype).min)\n        self.initialize(attn_mask, sequence, new_tokens_buffer, parents_buffer, position_ids, None)\n        self.set_prefix(prefix=prefix)\n        self.tree_size = self.grow_map[\"size\"]\n        print(self.tree_size)\n        self.tree_mask = tree_mask\n\n        self.full_attn_mask[self.max_length - self.tree_size + 1: self.max_length, self.max_length - self.tree_size + 1: self.max_length] = tree_mask[1:, 1:]\n\n\n        total_nodes = len(prefix) + self.tree_size - 1\n        self.attn_mask = self.full_attn_mask[self.max_length - total_nodes: 2 * self.max_length - total_nodes, self.max_length - total_nodes: 2 * self.max_length - total_nodes]\n        self.ground_truth_len = len(prefix)\n        \n        \n        self.position_ids[len(prefix) : len(prefix) + self.tree_size - 1] = (self.grow_map[\"depth\"][1:].to(self.device) + len(prefix) - 1)\n        self.storage_ids = torch.arange(self.max_length).to(self.device)\n        self.depth = self.grow_map[\"depth\"][1:].to(self.device)\n        \n        self.draft_logits = torch.zeros((self.max_length, vocab_size), dtype=self.dtype).to(self.device)\n        if draft_kv_len == 0:\n            draft_model_outputs = self.draft_model_engine.inference(input_ids=self.tokens[:self.num_nodes].unsqueeze(0), \n                                storage_ids=self.storage_ids[:self.num_nodes], \n                                position_ids=self.position_ids[:self.num_nodes].unsqueeze(0),\n                                attn_mask=self.attn_mask[:self.num_nodes][None, None, :, :])\n            self.draft_logits[0] :torch.FloatTensor= draft_model_outputs[...,-1,:][0]\n        \n        else:\n            draft_model_outputs = self.draft_model_engine.inference(input_ids = self.tokens[draft_kv_len: self.num_nodes].unsqueeze(0), \n                                                    storage_ids=self.storage_ids[draft_kv_len: self.num_nodes],\n                                                    position_ids=self.position_ids[draft_kv_len: self.num_nodes].unsqueeze(0),\n                                                    attn_mask=self.attn_mask[draft_kv_len: self.num_nodes][None, None, :, :])\n            self.draft_logits[0] :torch.FloatTensor = draft_model_outputs[...,-1,:][0]\n        self.draft_kv_len = self.num_nodes\n        \n        self.target_kv_len = target_kv_len\n    \n        self.seq_to_use = list(range(self.max_length))\n    \n    @torch.inference_mode()\n    def collective_grow_static(self, idx_list, n_branch_list :list[int], benchmark=False, grow_step = None):\n        \n        if benchmark:\n            x1 = 0.0\n            x2 = 0.0\n        \n        \n        \n        \n        total_branch = sum(n_branch_list)\n        max_branch = max(n_branch_list)\n\n        if benchmark:\n                torch.cuda.synchronize()\n                t1 = time.time()\n        #sampling_logits = self.draft_logits[idx_list]\n            \n        # new_tokens_set = sampling_logits.topk(k=max_branch).indices.flatten()\n        new_tokens_set = self.sampling_callables[grow_step](self.draft_logits[idx_list])\n        # finished_tokens = 0\n            \n        # for i in range(len(idx_list)):\n        #         n_branch = n_branch_list[i]\n        #         self.tokens[self.num_nodes + finished_tokens: self.num_nodes + finished_tokens + n_branch]  = new_tokens_set[i][:n_branch]\n        #         finished_tokens += n_branch\n        self.tokens[self.num_nodes: self.num_nodes + total_branch] = new_tokens_set[self.sample_gather_indices[grow_step]]\n            \n        if benchmark:\n                    torch.cuda.synchronize()\n                    t2 = time.time()\n                    x1 += (t2 - t1)\n        self.num_nodes = self.num_nodes + total_branch\n        \n        start_pos = self.num_nodes - total_branch\n        end_pos = self.num_nodes\n        attn_mask = self.attn_mask[self.num_nodes - total_branch: self.num_nodes]\n        attn_mask = attn_mask[None, None, :, :]\n        \n        draft_model_outputs = self.draft_model_engine.graph_inference(\n            input_ids = self.tokens[self.draft_kv_len: self.num_nodes].unsqueeze(0),\n            position_ids = self.position_ids[start_pos : end_pos].unsqueeze(0),\n            attn_mask = attn_mask,\n            storage_ids=self.storage_ids[self.draft_kv_len: self.num_nodes]\n            \n        )\n        self.draft_kv_len = self.num_nodes\n        self.draft_logits[start_pos - self.ground_truth_len + 1:end_pos - self.ground_truth_len + 1] = draft_model_outputs[0][-total_branch:]\n        if benchmark:\n                    torch.cuda.synchronize()\n                    t3 = time.time()\n                    x2 += (t3 - t2)\n        if benchmark:\n            return n_branch_list, x1, x2\n        return n_branch_list\n    @torch.inference_mode()\n    def collective_grow_dynamic(self, idx_list, n_branch_list :list[int], benchmark=False, grow_step = None):\n        \n        if benchmark:\n            x1 = 0.0\n            x2 = 0.0\n        \n        \n        \n        total_branch = sum(n_branch_list)\n        max_branch = max(n_branch_list)\n\n        if benchmark:\n                torch.cuda.synchronize()\n                t1 = time.time()\n        #sampling_logits = self.draft_logits[idx_list]\n            \n        # new_tokens_set = sampling_logits.topk(k=max_branch).indices.flatten()\n        new_tokens_set = self.sampling_callables[grow_step](self.draft_logits[idx_list])\n        # finished_tokens = 0\n            \n        # for i in range(len(idx_list)):\n        #         n_branch = n_branch_list[i]\n        #         self.tokens[self.num_nodes + finished_tokens: self.num_nodes + finished_tokens + n_branch]  = new_tokens_set[i][:n_branch]\n        #         finished_tokens += n_branch\n        self.tokens[self.num_nodes: self.num_nodes + total_branch] = new_tokens_set[self.sample_gather_indices[grow_step]]\n            \n        if benchmark:\n                    torch.cuda.synchronize()\n                    t2 = time.time()\n                    x1 += (t2 - t1)\n        self.num_nodes = self.num_nodes + total_branch\n        \n        start_pos = self.num_nodes - total_branch\n        end_pos = self.num_nodes\n        attn_mask = self.attn_mask[self.num_nodes - total_branch: self.num_nodes]\n        attn_mask = attn_mask[None, None, :, :]\n        \n        draft_model_outputs = self.draft_model_engine.graph_inference(\n            input_ids = self.tokens[self.draft_kv_len: self.num_nodes].unsqueeze(0),\n            position_ids = self.position_ids[start_pos : end_pos].unsqueeze(0),\n            attn_mask = attn_mask,\n            storage_ids=self.storage_ids[self.draft_kv_len: self.num_nodes]\n            \n        )\n        self.draft_kv_len = self.num_nodes\n        self.draft_logits[start_pos - self.ground_truth_len + 1:end_pos - self.ground_truth_len + 1] = draft_model_outputs[0][-total_branch:]\n        if benchmark:\n                    torch.cuda.synchronize()\n                    t3 = time.time()\n                    x2 += (t3 - t2)\n        if benchmark:\n            return n_branch_list, x1, x2\n        return n_branch_list\n    @torch.inference_mode()\n    def accept_step(self, parent_id :int) ->ChildrenAccept:\n        logits_id = parent_id - (self.ground_truth_len - 1)\n        \n        target_token = self.target_token[logits_id]\n        children = self.Successors[logits_id]\n        if len(children) == 0:\n            return -1\n        \n        for pos in children:\n\n            token = self.tokens[pos + (self.ground_truth_len - 1)]\n            if token == target_token:\n                return pos + (self.ground_truth_len - 1)\n        \n        return -1\n\n\n        \n    @torch.inference_mode()\n    def verify(self, benchmark = False):\n        new_node_num = (self.num_nodes - self.ground_truth_len + 1)\n        if self.target_kv_len == 0:\n            start_pos = 0\n            end_pos = self.num_nodes\n            attn_mask = self.attn_mask[start_pos: end_pos, :end_pos]\n            attn_mask = attn_mask[None, None, :, :]\n            if benchmark:\n                torch.cuda.synchronize()\n                t1 = time.time()\n            target_model_outputs = self.target_model_engine.inference(input_ids = self.tokens[start_pos : end_pos].unsqueeze(0), \n                                    position_ids = self.position_ids[start_pos : end_pos].unsqueeze(0), attn_mask = attn_mask, \n                                    storage_ids=self.storage_ids[start_pos : end_pos])\n            if benchmark:\n                torch.cuda.synchronize()\n                t2 = time.time()\n            self.target_logits :torch.FloatTensor= target_model_outputs[0][self.ground_truth_len - 1:]\n            \n        else:\n            start_pos = self.target_kv_len\n            end_pos = self.num_nodes\n            attn_mask = self.attn_mask[start_pos: end_pos, :end_pos]\n            attn_mask = attn_mask[None, None, :, :]\n            if benchmark:\n                torch.cuda.synchronize()\n                t1 = time.time()\n            target_model_outputs = self.target_model_engine.inference(input_ids = self.tokens[start_pos : end_pos].unsqueeze(0), \n                                        position_ids =self.position_ids[start_pos : end_pos].unsqueeze(0), attn_mask = attn_mask,\n                                        storage_ids=self.storage_ids[start_pos : end_pos])\n            if benchmark:\n                torch.cuda.synchronize()\n                t2 = time.time()\n            self.target_logits :torch.FloatTensor = target_model_outputs[0][-(new_node_num):]\n        \n        self.target_logits = get_sampling_logits(logits=self.target_logits, top_p=self.top_p, T=self.temperature, replicate=False)\n        self.target_logits = softmax(self.target_logits / self.temperature, dim=-1)\n        #self.target_token = self.target_logits.argmax(dim=-1)\n        self.target_token = self.target_logits.multinomial(num_samples=1)\n        accept_list = self.seq_to_use[:self.ground_truth_len]\n        \n        terminal = False\n        while True:\n            parent_id = accept_list[-1]\n            pos = self.accept_step(parent_id=parent_id)\n            if pos != -1:\n                accept_list.append(pos)\n                if self.tokens[pos] == 0 or self.tokens[pos] == 2:\n                     terminal = True\n                     break\n            else:\n                break\n        if benchmark:\n            torch.cuda.synchronize()\n            t3 = time.time()\n        accept_length = len(accept_list)\n        self.tokens[:accept_length] = self.tokens[accept_list]\n        if not terminal:\n            self.tokens[accept_length] = self.target_token[accept_list[-1] - self.ground_truth_len + 1].reshape(1)\n            self.draft_model_engine.engine.kv_cache.gather_kv_incremental(accept_list[self.ground_truth_len:], self.ground_truth_len)\n            self.target_model_engine.engine.kv_cache.gather_kv_incremental(accept_list[self.ground_truth_len:], self.ground_truth_len)\n            if benchmark:\n                torch.cuda.synchronize()\n                t4 = time.time()\n                self.prepare_for_next_iter(accept_list, self.tokens[:accept_length+1])\n                return self.tokens[:accept_length+1], accept_length, accept_length, t2 - t1, t3-t2, t4 - t3, terminal\n            self.prepare_for_next_iter(accept_list, self.tokens[:accept_length+1])\n            return self.tokens[:accept_length+1], accept_length, accept_length, terminal\n            \n        else:\n             if benchmark:\n                torch.cuda.synchronize()\n                t4 = time.time()\n                return self.tokens[:accept_length], accept_length, accept_length, t2 - t1, t3-t2, t4 - t3, terminal\n             return self.tokens[:accept_length], accept_length, accept_length, terminal\n    def verbose(self):\n        super().verbose()\n    def construct_grow_map(self, benchmark = False):\n        if benchmark:\n            sample_time = 0\n            compute_time = 0\n        for i in range(self.draft_step - 1):\n                if benchmark:\n                        _, t1, t2 = self.collective_grow_static(self.grow_map_roots_gpu[i], self.grow_map['branches'][i], benchmark=benchmark, grow_step=i)\n                        sample_time += t1\n                        compute_time += t2   \n                else:\n                        self.collective_grow_static(self.grow_map_roots_gpu[i], self.grow_map['branches'][i], grow_step=i)\n        if benchmark:\n            return sample_time, compute_time\n        else:\n            return None\n    def construct_dynamic_tree(self, benchmark = False):\n        if benchmark:\n            sample_time = 0\n            compute_time = 0\n        for i in range(self.draft_step - 1):\n                if benchmark:\n                        _, t1, t2 = self.collective_grow_static(self.grow_map_roots_gpu[i], self.grow_map['branches'][i], benchmark=benchmark, grow_step=i)\n                        sample_time += t1\n                        compute_time += t2   \n                else:\n                        self.collective_grow_dynamic(self.grow_map_roots_gpu[i], self.grow_map['branches'][i],grow_step=i)\n        if benchmark:\n            return sample_time, compute_time\n        else:\n            return None\n    def prepare_for_next_iter(self, accept_list: list[int], valid_tokens :torch.LongTensor):\n        if len(accept_list) + 1 > self.max_target_seq:\n              return \n        # self.tokens[:len(valid_tokens)] = valid_tokens\n        self.position_ids[:len(accept_list)] =  self.position_ids[accept_list]\n        self.position_ids[len(accept_list)] = len(accept_list) \n        self.position_ids[len(valid_tokens) : len(valid_tokens) + self.tree_size - 1] = (self.depth + len(valid_tokens) - 1)\n        self.ground_truth_len = len(valid_tokens)\n        self.num_nodes = len(valid_tokens)\n\n        total_nodes = len(valid_tokens) + self.tree_size - 1\n        # self.attn_mask.fill_(torch.finfo(self.dtype).min)\n        # self.attn_mask[:self.num_nodes, :self.num_nodes] = _make_causal_mask((1, self.num_nodes),dtype=self.dtype, device=self.device)\n        # self.attn_mask[len(valid_tokens) : len(valid_tokens) + self.tree_size - 1, : len(valid_tokens)] = 0.0\n        # self.attn_mask[len(valid_tokens) : len(valid_tokens) + self.tree_size - 1, len(valid_tokens) : len(valid_tokens) + self.tree_size - 1] = self.tree_mask[1:, 1:]\n\n        self.attn_mask = self.full_attn_mask[self.max_length - total_nodes: 2 * self.max_length - total_nodes, self.max_length - total_nodes: 2 * self.max_length - total_nodes]\n\n        \n        draft_model_outputs = self.draft_model_engine.graph_inference(input_ids = self.tokens[len(accept_list): self.num_nodes].unsqueeze(0), \n                                                    storage_ids=self.storage_ids[len(accept_list): self.num_nodes],\n                                                    position_ids=self.position_ids[len(accept_list): self.num_nodes].unsqueeze(0),\n                                                    attn_mask=self.attn_mask[len(accept_list): self.num_nodes][None, None, :, :])\n        \n        self.draft_logits[0] :torch.FloatTensor = draft_model_outputs[...,-1,:][0]\n\n        self.draft_kv_len = self.num_nodes\n        self.target_kv_len = len(accept_list)\n        \n\n\n        \n\n\nclass GreedySTreeTest(Tree):\n    def __init__(self, \n                 draft_model_engine :GraphInferenceEngine,\n                 target_model_engine :GraphInferenceEngineTG,\n                 prefix :torch.LongTensor,\n                 temperature :float = 0.6,\n                 top_p: float = 0.9,\n                 draft_kv_len = 0,\n                 target_kv_len = 0,\n                 max_length = 256,\n                 max_width = 32,\n                 device :str = 'cpu',\n                 attn_mask = None, \n                 sequence = None, \n                 new_tokens_buffer = None, \n                 parents_buffer = None, \n                 position_ids = None) -> None:\n        \n        super().__init__(device=device, max_length=max_length)\n        assert self.max_length == draft_model_engine.engine.max_length\n        self.max_width = max_width\n        self.draft_model_engine = draft_model_engine\n        self.target_model_engine = target_model_engine\n        self.temperature = temperature\n        self.top_p = top_p\n        \n        self.initialize(attn_mask, sequence, new_tokens_buffer, parents_buffer, position_ids, None)\n        self.set_prefix(prefix=prefix)\n        self.Successors = [list(range(1, self.max_width + 1))]\n        self.Successors.extend([[] for _ in range(self.max_width)])\n\n        self.attn_mask = self.full_attn_mask[:self.max_length, :self.max_length]\n        for idx in range(self.max_width):\n             self.attn_mask[idx + self.num_nodes] = self.attn_mask[self.num_nodes - 1]\n             self.attn_mask[idx + self.num_nodes][idx + self.num_nodes] = 0.0\n        \n        self.position_ids[self.num_nodes : self.num_nodes + self.max_width] = self.position_ids[self.num_nodes - 1] + 1\n        self.ground_truth_len = len(prefix)\n        self.r = torch.rand(len(position_ids)).to(self.device)\n        self.storage_ids = torch.arange(self.max_length).to(self.device)\n        \n        \n        if draft_kv_len == 0:\n            draft_model_outputs = self.draft_model_engine.inference(input_ids=self.tokens[:self.num_nodes].unsqueeze(0), \n                                storage_ids=self.storage_ids[:self.num_nodes], \n                                position_ids=self.position_ids[:self.num_nodes].unsqueeze(0),\n                                attn_mask=self.attn_mask[:self.num_nodes][None, None, :, :])\n            self.draft_logits :torch.FloatTensor= draft_model_outputs[...,-1,:]\n        \n        else:\n            draft_model_outputs = self.draft_model_engine.inference(input_ids = self.tokens[draft_kv_len: self.num_nodes].unsqueeze(0), \n                                                    storage_ids=self.storage_ids[draft_kv_len: self.num_nodes],\n                                                    position_ids=self.position_ids[draft_kv_len: self.num_nodes].unsqueeze(0),\n                                                    attn_mask=self.attn_mask[draft_kv_len: self.num_nodes][None, None, :, :])\n            self.draft_logits :torch.FloatTensor = draft_model_outputs[...,-1,:]\n        self.draft_kv_len = self.num_nodes\n        \n        self.target_kv_len = target_kv_len\n        self.rand = torch.empty((self.max_width + 1, self.draft_logits.shape[1])).uniform_().to(self.device)\n        self.collective_grow_static([0], [self.max_width])\n    \n    @torch.inference_mode()\n    def collective_grow_static(self, idx_list :torch.LongTensor, n_branch_list :list[int], benchmark=False):\n        \n        \n        assert len(set(idx_list)) == len(idx_list)\n        assert len(self.draft_logits) == (self.num_nodes - self.ground_truth_len + 1)\n        \n        total_branch = sum(n_branch_list)\n        max_branch = max(n_branch_list)\n        sampling_logits = self.draft_logits[idx_list]\n        \n        sampling_q = softmax(sampling_logits / self.temperature, dim=-1)\n        \n            \n            \n        new_tokens_set  = (self.rand[idx_list].log()/sampling_q).topk(k=max_branch).indices\n        \n            \n        \n        finished_tokens = 0\n            \n        for i, idx in enumerate(idx_list):\n                n_branch = n_branch_list[i]\n                self.tokens[self.num_nodes + finished_tokens: self.num_nodes + finished_tokens + n_branch]  = new_tokens_set[i][:n_branch]\n                finished_tokens += n_branch\n            \n        \n        self.num_nodes = self.num_nodes + total_branch\n        \n\n        \n        start_pos = self.num_nodes - total_branch\n        end_pos = self.num_nodes\n        attn_mask = self.attn_mask[self.num_nodes - total_branch: self.num_nodes]\n        attn_mask = attn_mask[None, None, :, :]\n        \n        draft_model_outputs = self.draft_model_engine.graph_inference(\n            input_ids = self.tokens[self.draft_kv_len: self.num_nodes].unsqueeze(0),\n            position_ids = self.position_ids[start_pos : end_pos].unsqueeze(0),\n            attn_mask = attn_mask,\n            storage_ids=self.storage_ids[self.draft_kv_len: self.num_nodes]\n            \n        )\n        self.draft_kv_len = self.num_nodes\n        self.draft_logits = torch.cat([self.draft_logits, draft_model_outputs[0][-total_branch:]], dim=0)\n        assert len(self.draft_logits) == (self.num_nodes - self.ground_truth_len + 1)\n        \n        return n_branch_list\n    @torch.inference_mode()\n    def accept_step(self, parent_id :int) ->ChildrenAccept:\n        logits_id = parent_id - (self.ground_truth_len - 1)\n        p = self.target_logits[logits_id]\n        \n        draft_logits = self.draft_logits[logits_id]\n        children = self.Successors[logits_id]\n        if len(children) == 0:\n            return ChildrenAccept(accept_mark=2, residual=p)\n        \n        for idx, pos in enumerate(children):\n\n            token = self.tokens[pos + (self.ground_truth_len - 1)]\n            q = softmax(draft_logits / self.temperature, dim=-1)\n            r = self.r[pos + (self.ground_truth_len - 1)]\n            if p[token] >= r * q[token]:\n                return ChildrenAccept(accept_mark=0, token=token, position=pos + (self.ground_truth_len - 1), successor_order=idx)\n            else:\n                p = get_residual(p, q)\n                draft_logits[token] = -torch.inf\n        \n        return ChildrenAccept(accept_mark=1, residual=p)\n\n\n        \n    @torch.inference_mode()\n    def verify(self, benchmark = False):\n        new_node_num = (self.num_nodes - self.ground_truth_len + 1)\n        if self.target_kv_len == 0:\n            start_pos = 0\n            end_pos = self.num_nodes\n            attn_mask = self.attn_mask[start_pos: end_pos, :end_pos]\n            attn_mask = attn_mask[None, None, :, :].type(self.target_model_engine.dtype)\n            target_model_outputs = self.target_model_engine.inference(input_ids = self.tokens[start_pos : end_pos].unsqueeze(0), \n                                    position_ids = self.position_ids[start_pos : end_pos].unsqueeze(0), attn_mask = attn_mask, \n                                    storage_ids=self.storage_ids[start_pos : end_pos])\n            self.target_logits :torch.FloatTensor= target_model_outputs[0][self.ground_truth_len - 1:]\n            \n        else:\n            start_pos = self.target_kv_len\n            end_pos = self.num_nodes\n            attn_mask = self.attn_mask[start_pos: end_pos, :end_pos]\n            attn_mask = attn_mask[None, None, :, :].type(self.target_model_engine.dtype)\n            target_model_outputs = self.target_model_engine.inference(input_ids = self.tokens[start_pos : end_pos].unsqueeze(0), \n                                        position_ids =self.position_ids[start_pos : end_pos].unsqueeze(0), attn_mask = attn_mask,\n                                        storage_ids=self.storage_ids[start_pos : end_pos])\n            \n            self.target_logits :torch.FloatTensor = target_model_outputs[0][-(new_node_num):]\n        \n        assert len(self.draft_logits) == (self.num_nodes - self.ground_truth_len + 1)\n        assert len(self.target_logits) == (self.num_nodes - self.ground_truth_len + 1)\n        self.target_logits = get_sampling_logits(logits=self.target_logits, top_p=self.top_p, T=self.temperature, replicate=False)\n        self.target_logits = softmax(self.target_logits / self.temperature, dim=-1)\n        accept_list = list(range(self.ground_truth_len))\n        b = -1\n        terminal = False\n        while True:\n            parent_id = accept_list[-1]\n            children_accept = self.accept_step(parent_id=parent_id)\n            if children_accept.accept_mark == 0:\n                accept_list.append(children_accept.position)\n                b = children_accept.successor_order\n                if self.tokens[children_accept.position] == 2 or self.tokens[children_accept.position] == 0:\n                     terminal = True\n                     break\n            else:\n                residual = children_accept.residual\n                break\n        if not terminal:\n            if torch.isnan(residual).any():\n                 terminal = True\n            else:\n                last_token = residual.multinomial(num_samples=1, replacement=True)\n\n        \n        accept_tokens = self.tokens[accept_list]\n        if not terminal:\n            valid_tokens = torch.cat([accept_tokens, last_token], dim=-1)\n            \n            self.draft_model_engine.gather_kv(accept_list)\n            self.target_model_engine.gather_kv(accept_list)\n\n            return valid_tokens, len(accept_list), len(accept_list), b, terminal\n        else:\n            return accept_tokens, len(accept_list), len(accept_list), b, terminal\n    \n    def verbose(self):\n        super().verbose()\n\n    \n    \n\n                "
        }
    ]
}