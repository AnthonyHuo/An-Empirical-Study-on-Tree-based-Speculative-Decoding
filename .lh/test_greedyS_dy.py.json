{
    "sourceFile": "test_greedyS_dy.py",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 8,
            "patches": [
                {
                    "date": 1712788622127,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                },
                {
                    "date": 1712788644244,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -227,11 +227,11 @@\n #tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n if args.dataset == 'openwebtext':\n     tokenized_dataset_eval = load_from_disk(\"dataset/openwebtext_eval\").select(list(range(args.start, args.end)))\n elif args.dataset == 'wiki':\n-    tokenized_dataset_eval = convert_wiki_dataset(tokenizer=tokenizer).select(eval_list[args.start :args.end])\n+    tokenized_dataset_eval = convert_wiki_dataset(tokenizer=tokenizer).select(list(range(args.start, args.end)))\n elif args.dataset == 'cnn':\n-    tokenized_dataset_eval = convert_cnn_dataset(tokenizer=tokenizer).select(eval_list[args.start :args.end])\n+    tokenized_dataset_eval = convert_cnn_dataset(tokenizer=tokenizer).select(list(range(args.start, args.end)))\n else:\n     tokenized_dataset_eval = convert_dataset(tokenizer=tokenizer,file_path=args.dataset).select(list(range(args.start, args.end)))\n \n data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\n"
                },
                {
                    "date": 1712788699112,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -16,8 +16,9 @@\n from time import sleep\n from utils import get_sampling_logits, _make_causal_mask, get_residual, cuda_graph_for_residual, cuda_graph_for_sampling_without_replacement, cuda_graph_for_sampling_with_replacement,cuda_graph_for_sampling_argmax \n import json\n from Engine import GraphInferenceEngine, GraphInferenceEngineTG\n+from data_converter import convert_wiki_dataset, convert_cnn_dataset, convert_c4_dataset_eval\n parser = argparse.ArgumentParser()\n parser.add_argument('--model', type=str, help='model')\n parser.add_argument('--target', type=str, help='target model')\n parser.add_argument('--dataset', type=str, default=\"dataset/c4_small.json\", help='dataset path')\n"
                },
                {
                    "date": 1712788704478,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -16,9 +16,9 @@\n from time import sleep\n from utils import get_sampling_logits, _make_causal_mask, get_residual, cuda_graph_for_residual, cuda_graph_for_sampling_without_replacement, cuda_graph_for_sampling_with_replacement,cuda_graph_for_sampling_argmax \n import json\n from Engine import GraphInferenceEngine, GraphInferenceEngineTG\n-from data_converter import convert_wiki_dataset, convert_cnn_dataset, convert_c4_dataset_eval\n+from data_converter import convert_wiki_dataset, convert_cnn_dataset\n parser = argparse.ArgumentParser()\n parser.add_argument('--model', type=str, help='model')\n parser.add_argument('--target', type=str, help='target model')\n parser.add_argument('--dataset', type=str, default=\"dataset/c4_small.json\", help='dataset path')\n"
                },
                {
                    "date": 1712789122001,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -230,9 +230,9 @@\n     tokenized_dataset_eval = load_from_disk(\"dataset/openwebtext_eval\").select(list(range(args.start, args.end)))\n elif args.dataset == 'wiki':\n     tokenized_dataset_eval = convert_wiki_dataset(tokenizer=tokenizer).select(list(range(args.start, args.end)))\n elif args.dataset == 'cnn':\n-    tokenized_dataset_eval = convert_cnn_dataset(tokenizer=tokenizer).select(list(range(args.start, args.end)))\n+    # tokenized_dataset_eval = convert_cnn_dataset(tokenizer=tokenizer).select(list(range(args.start, args.end)))\n else:\n     tokenized_dataset_eval = convert_dataset(tokenizer=tokenizer,file_path=args.dataset).select(list(range(args.start, args.end)))\n \n data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\n"
                },
                {
                    "date": 1712789129272,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -230,8 +230,9 @@\n     tokenized_dataset_eval = load_from_disk(\"dataset/openwebtext_eval\").select(list(range(args.start, args.end)))\n elif args.dataset == 'wiki':\n     tokenized_dataset_eval = convert_wiki_dataset(tokenizer=tokenizer).select(list(range(args.start, args.end)))\n elif args.dataset == 'cnn':\n+    tokenized_dataset_eval = load_from_disk(\"dataset/openwebtext_eval\").select(list(range(args.start, args.end)))\n     # tokenized_dataset_eval = convert_cnn_dataset(tokenizer=tokenizer).select(list(range(args.start, args.end)))\n else:\n     tokenized_dataset_eval = convert_dataset(tokenizer=tokenizer,file_path=args.dataset).select(list(range(args.start, args.end)))\n \n"
                },
                {
                    "date": 1712789144279,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -230,9 +230,9 @@\n     tokenized_dataset_eval = load_from_disk(\"dataset/openwebtext_eval\").select(list(range(args.start, args.end)))\n elif args.dataset == 'wiki':\n     tokenized_dataset_eval = convert_wiki_dataset(tokenizer=tokenizer).select(list(range(args.start, args.end)))\n elif args.dataset == 'cnn':\n-    tokenized_dataset_eval = load_from_disk(\"dataset/openwebtext_eval\").select(list(range(args.start, args.end)))\n+    tokenized_dataset_eval = load_from_disk(\"dataset/cnn\").select(list(range(args.start, args.end)))\n     # tokenized_dataset_eval = convert_cnn_dataset(tokenizer=tokenizer).select(list(range(args.start, args.end)))\n else:\n     tokenized_dataset_eval = convert_dataset(tokenizer=tokenizer,file_path=args.dataset).select(list(range(args.start, args.end)))\n \n"
                },
                {
                    "date": 1712789303949,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -228,9 +228,10 @@\n #tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n if args.dataset == 'openwebtext':\n     tokenized_dataset_eval = load_from_disk(\"dataset/openwebtext_eval\").select(list(range(args.start, args.end)))\n elif args.dataset == 'wiki':\n-    tokenized_dataset_eval = convert_wiki_dataset(tokenizer=tokenizer).select(list(range(args.start, args.end)))\n+    tokenized_dataset_eval = load_from_disk(\"dataset/cnn\").select(list(range(args.start, args.end)))\n+    # tokenized_dataset_eval = convert_wiki_dataset(tokenizer=tokenizer).select(list(range(args.start, args.end)))\n elif args.dataset == 'cnn':\n     tokenized_dataset_eval = load_from_disk(\"dataset/cnn\").select(list(range(args.start, args.end)))\n     # tokenized_dataset_eval = convert_cnn_dataset(tokenizer=tokenizer).select(list(range(args.start, args.end)))\n else:\n"
                },
                {
                    "date": 1712789314431,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -228,9 +228,9 @@\n #tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n if args.dataset == 'openwebtext':\n     tokenized_dataset_eval = load_from_disk(\"dataset/openwebtext_eval\").select(list(range(args.start, args.end)))\n elif args.dataset == 'wiki':\n-    tokenized_dataset_eval = load_from_disk(\"dataset/cnn\").select(list(range(args.start, args.end)))\n+    tokenized_dataset_eval = load_from_disk(\"dataset/wikipedia\").select(list(range(args.start, args.end)))\n     # tokenized_dataset_eval = convert_wiki_dataset(tokenizer=tokenizer).select(list(range(args.start, args.end)))\n elif args.dataset == 'cnn':\n     tokenized_dataset_eval = load_from_disk(\"dataset/cnn\").select(list(range(args.start, args.end)))\n     # tokenized_dataset_eval = convert_cnn_dataset(tokenizer=tokenizer).select(list(range(args.start, args.end)))\n"
                }
            ],
            "date": 1712788622127,
            "name": "Commit-0",
            "content": "from transformers import LlamaForCausalLM, LlamaTokenizer, DataCollatorForLanguageModeling, OPTForCausalLM, AutoTokenizer\nimport torch\nimport numpy as np \nfrom datasets import load_from_disk, Dataset\nfrom torch.utils.data.dataloader import DataLoader\nfrom tqdm import tqdm\nfrom torch.nn.functional import softmax\nimport accelerate\nfrom accelerate import Accelerator\nimport argparse\nfrom data_converter import convert_dataset\nimport argparse\nfrom GreedySTree_dy import GreedySTree\nfrom Llama import LlamaForCausalLM_Attn\nimport time\nfrom time import sleep\nfrom utils import get_sampling_logits, _make_causal_mask, get_residual, cuda_graph_for_residual, cuda_graph_for_sampling_without_replacement, cuda_graph_for_sampling_with_replacement,cuda_graph_for_sampling_argmax \nimport json\nfrom Engine import GraphInferenceEngine, GraphInferenceEngineTG\nparser = argparse.ArgumentParser()\nparser.add_argument('--model', type=str, help='model')\nparser.add_argument('--target', type=str, help='target model')\nparser.add_argument('--dataset', type=str, default=\"dataset/c4_small.json\", help='dataset path')\nparser.add_argument('--growmap', type=str, default=\"growmaps/68m_7b-64.pt\", help='dataset path')\nparser.add_argument('--start', type=int, default=0, help='start')\nparser.add_argument('--end', type=int, default=200, help='end')\nparser.add_argument('--T', type=float, default=0.6, help='temperature')\nparser.add_argument('--P', type=float, default=0.9, help='top_p')\nparser.add_argument('--DP', type=float, default=1.1, help='draft_top_p')\nparser.add_argument('--D', type=int, default=1, help='depth')\nparser.add_argument('--B', type=int, default=16, help='budget')\nparser.add_argument('--W', type=int, default=16, help='max width')\nparser.add_argument('--M', type=int, default=256, help='max length')\nparser.add_argument('--Mode', type=str, default=\"greedy\", help='tree mode')\nparser.add_argument('--decay', type=float, default=0.85, help='decay')\nparser.add_argument('--negative', action='store_true')\nparser.add_argument('--static', action='store_true')\nparser.add_argument('--offloading', action='store_true')\nargs = parser.parse_args()\nprint(args)\n\n\n\ndef simulation_greedy_with_tree_fast(target_model : GraphInferenceEngineTG, draft_model: GraphInferenceEngine, dataloader: DataLoader, T=0.6, top_p=0.9, \n            draft_top_p=1.1, budget=32, w=4, decay=0.85, negative=False, static=False, \n            max_length=512, residual_graph=None, grow_map=None, sampling_callables = None,\n            sample_gather_indices = None):\n    num_eval_steps = len(dataloader)\n    num_decoding_steps = 0\n    num_large_model_steps = 0\n    total_time = 0.0\n    dtype = torch.float16\n    attn_mask = torch.full((max_length, max_length), torch.finfo(dtype).min, dtype=dtype, device='cuda:0')\n    sequence = torch.tensor(list(range(max_length)), device='cuda:0').long().unsqueeze(-1)\n    new_tokens_buffer =  torch.zeros(max_length).long().to('cuda:0')\n    parents_buffer =  torch.zeros(max_length).long().to('cuda:0')\n    position_ids = torch.zeros(max_length).long().to('cuda:0')\n    \n    with torch.no_grad():\n        for step, batch in tqdm(enumerate(dataloader), total=num_eval_steps):\n            input_ids = batch['input_ids'][..., :128]\n            labels = batch['labels'][..., :128]\n            terminate = False\n            if labels[0][-1] == -100: terminate = True\n            draft_kv_len = 0\n            target_kv_len = 0\n            attn_mask.fill_(torch.finfo(dtype).min)\n            spectree = GreedySTree(prefix=input_ids.squeeze(0), device='cuda:0', temperature=T,\n                                    top_p=top_p,\n                                    draft_kv_len=draft_kv_len, target_kv_len=target_kv_len,\n                                    draft_model_engine=draft_model, target_model_engine=target_model, max_length=max_length, grow_map=grow_map,\n                                    attn_mask = attn_mask, sequence = sequence, new_tokens_buffer = new_tokens_buffer, \n                                    parents_buffer = parents_buffer, \n                                    position_ids = position_ids,\n                                    residual_graph = residual_graph,\n                                    sampling_callables=sampling_callables,\n                                    sample_gather_indices = sample_gather_indices)\n            torch.cuda.synchronize()\n            t1 = time.time()\n            while input_ids.shape[1] < 256 and terminate == False:\n                # spectree.construct_grow_map()\n                spectree.construct_dynamic_tree()\n                valid_tokens, draft_kv_len, target_kv_len, terminate = spectree.verify()\n                \n                num_decoding_steps += (valid_tokens.shape[0] - input_ids.shape[1])\n                num_large_model_steps += 1\n                input_ids = valid_tokens.unsqueeze(0)\n                if (input_ids[0][-1] == 2) or (input_ids[0][-1] == 0): terminate = True\n            \n            torch.cuda.synchronize()\n            t2 = time.time()\n            total_time += (t2 - t1)\n            draft_model.clear_kv()\n            target_model.clear_kv()\n    print(\"total time :{:.5f}s, latency :{:.5f}s, decoding step: {}, large model step: {}\".format(total_time, total_time / num_decoding_steps, num_decoding_steps, num_large_model_steps))\n    return num_decoding_steps / num_large_model_steps\n\n\n\ndef simulation_baseline(target_model : GraphInferenceEngineTG, dataloader: DataLoader, T=0.6, top_p=0.9, max_length=256):\n    num_eval_steps = len(dataloader)\n    num_decoding_steps = 0\n    total_time = 0.0\n    with torch.no_grad():\n        for step, batch in tqdm(enumerate(dataloader), total=num_eval_steps):\n            input_ids = batch['input_ids'][..., :128]\n            labels = batch['labels'][..., :128]\n            terminate = False\n            if labels[0][-1] == -100: terminate = True\n            position_ids = torch.arange(max_length).to('cuda:0').unsqueeze(0)\n            storage_ids = torch.arange(max_length).to('cuda:0')\n            attn_mask = _make_causal_mask((max_length, max_length), target_model.dtype, target_model.device)\n            torch.cuda.synchronize()\n            t1 = time.time()\n            inner_decoding_step = 0\n            start_length = 0\n            while inner_decoding_step < 128 and terminate == False:\n                if inner_decoding_step == 0:\n                    start_length = input_ids.shape[1]\n                    logits = target_model.inference(input_ids = input_ids, storage_ids=storage_ids[:start_length],\n                                                    position_ids = position_ids[..., :start_length], \n                                                    attn_mask=attn_mask[:start_length, :start_length][None, None, :, :])[0][-1]\n                    \n                else:\n                    logits = target_model.inference(input_ids = input_ids, storage_ids=storage_ids[start_length + inner_decoding_step-1 : start_length + inner_decoding_step],\n                                                    position_ids = position_ids[..., start_length + inner_decoding_step-1 : start_length + inner_decoding_step], \n                                                    attn_mask=attn_mask[start_length + inner_decoding_step-1 : start_length + inner_decoding_step, :start_length + inner_decoding_step][None, None, :, :])[0][-1]\n                \n                logits = get_sampling_logits(logits=logits, top_p=top_p, T=T)\n                \n                p = softmax(logits / T, dim=-1)\n                new_token = p.multinomial(num_samples=1).unsqueeze(0)\n                input_ids = new_token\n                num_decoding_steps += 1\n                inner_decoding_step += 1\n                if input_ids[0][-1] == 2: \n                    terminate = True\n            torch.cuda.synchronize()\n            t2 = time.time()\n            total_time += (t2 - t1)\n            target_model.clear_kv()\n            \n    print(\"total time :{:.5f}s, latency :{:.5f}s, decoding step: {}\".format(total_time, total_time / num_decoding_steps, num_decoding_steps))\n    return num_decoding_steps\ndef simulation_greedy_with_tree_fast_benchmark(target_model : GraphInferenceEngineTG, draft_model: GraphInferenceEngine, dataloader: DataLoader, T=0.6, top_p=0.9, \n                draft_top_p=1.1, budget=32, w=4, decay=0.85, negative=False, static=False, \n                max_length=512, residual_graph=None, grow_map=None, sampling_callables = None,\n                sample_gather_indices = None):\n    num_eval_steps = len(dataloader)\n    num_decoding_steps = 0\n    num_large_model_steps = 0\n    initialize_time = 0.0\n    speculate_time = 0.0\n    verify_time = 0.0\n    large_model_run = 0.0\n    accept_loop = 0.0\n    kv_select = 0.0\n    sample_time = 0.0\n    small_model_compute = 0.0\n    dtype = torch.float16\n    attn_mask = torch.full((max_length, max_length), torch.finfo(dtype).min, dtype=dtype, device='cuda:0')\n    sequence = torch.tensor(list(range(max_length)), device='cuda:0').long().unsqueeze(-1)\n    new_tokens_buffer =  torch.zeros(max_length).long().to('cuda:0')\n    parents_buffer =  torch.zeros(max_length).long().to('cuda:0')\n    position_ids = torch.zeros(max_length).long().to('cuda:0')\n    \n    with torch.no_grad():\n        for step, batch in tqdm(enumerate(dataloader), total=num_eval_steps):\n            input_ids = batch['input_ids'][..., :128]\n            labels = batch['labels'][..., :128]\n            terminate = False\n            if labels[0][-1] == -100: terminate = True\n            draft_kv_len = 0\n            target_kv_len = 0\n            attn_mask.fill_(torch.finfo(dtype).min)\n            spectree = GreedySTree(prefix=input_ids.squeeze(0), device='cuda:0', temperature=T,\n                                        top_p=top_p, \n                                        draft_kv_len=draft_kv_len, target_kv_len=target_kv_len,\n                                        draft_model_engine=draft_model, target_model_engine=target_model, max_length=max_length, grow_map=grow_map,\n                                        attn_mask = attn_mask, sequence = sequence, new_tokens_buffer = new_tokens_buffer, \n                                        parents_buffer = parents_buffer, \n                                        position_ids = position_ids,\n                                        residual_graph = residual_graph,\n                                        sampling_callables=sampling_callables,\n                                        sample_gather_indices = sample_gather_indices)\n            while input_ids.shape[1] < 256 and terminate == False:\n                torch.cuda.synchronize()\n                t1 = time.time()\n                torch.cuda.synchronize()\n                t2 = time.time()\n                a, b = spectree.construct_grow_map(benchmark=True)\n                torch.cuda.synchronize()\n                t3 = time.time()\n                valid_tokens, draft_kv_len, target_kv_len, x, y, z, terminate = spectree.verify(benchmark=True)\n                torch.cuda.synchronize()\n                t4 = time.time()\n                initial_size = input_ids.shape[1]\n                input_ids = valid_tokens.unsqueeze(0)\n                \n                if (input_ids[0] == 2)._is_any_true() or (input_ids[0] == 0)._is_any_true() or input_ids.shape[1] >= 256: \n                    terminate = True\n                if not terminate:\n                    sample_time += a\n                    small_model_compute += b\n                    large_model_run += x\n                    accept_loop += y\n                    kv_select += z\n                    initialize_time += (t2 - t1)\n                    speculate_time += (t3 - t2)\n                    verify_time += (t4 - t3)\n                    num_decoding_steps += (valid_tokens.shape[0] - initial_size)\n                    num_large_model_steps += 1\n            draft_model.clear_kv()\n            target_model.clear_kv()\n            if num_large_model_steps > 0:\n                print(num_decoding_steps / num_large_model_steps)\n    print(\"total decoding steps: {}\".format(num_decoding_steps), \"large model steps: {}\".format(num_large_model_steps), \"avg decoding step: {}\".format(num_decoding_steps / num_large_model_steps))\n    print(\"initialization time:{}\".format(initialize_time / num_large_model_steps), \"speculate time: {}\".format(speculate_time / num_large_model_steps),  \"verify time: {}\".format(verify_time / num_large_model_steps))\n    print(\"large model run: {}\".format(large_model_run / num_large_model_steps) , \"accept loop: {}\".format(accept_loop / num_large_model_steps), \"kv select: {}\".format(kv_select / num_large_model_steps))\n    print(\"small model run: {}\".format(small_model_compute / num_large_model_steps) , \"sample time: {}\".format(sample_time / num_large_model_steps))\n    return num_decoding_steps / num_large_model_steps\n\n\n\ntokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\", use_fast=False)\ntokenizer.pad_token = tokenizer.eos_token\n#tokenizer.add_special_tokens({'pad_token': '[PAD]'})\nif args.dataset == 'openwebtext':\n    tokenized_dataset_eval = load_from_disk(\"dataset/openwebtext_eval\").select(list(range(args.start, args.end)))\nelif args.dataset == 'wiki':\n    tokenized_dataset_eval = convert_wiki_dataset(tokenizer=tokenizer).select(eval_list[args.start :args.end])\nelif args.dataset == 'cnn':\n    tokenized_dataset_eval = convert_cnn_dataset(tokenizer=tokenizer).select(eval_list[args.start :args.end])\nelse:\n    tokenized_dataset_eval = convert_dataset(tokenizer=tokenizer,file_path=args.dataset).select(list(range(args.start, args.end)))\n\ndata_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\ndataloader = DataLoader(tokenized_dataset_eval, batch_size=1, collate_fn=data_collator, shuffle=False)\n\n\nif args.Mode == 'baseline':\n    target_model =  GraphInferenceEngineTG(max_length=args.M, model_name_or_path = args.target, dtype = torch.float16, device=\"cuda:0\", offloading=args.offloading)\nelse:\n    draft_model = GraphInferenceEngine(max_length=args.M, model_name_or_path = args.model, dtype = torch.float16, device=\"cuda:0\")\n    target_model = GraphInferenceEngineTG(max_length=args.M, model_name_or_path = args.target, dtype = torch.float16, device=\"cuda:0\", offloading=args.offloading)\n    graph_capture_list = list(range(1, 129))\n    draft_model.initialize_cuda_graph(graph_capture_list)\n    residual_graph = cuda_graph_for_residual()\n    path = args.growmap\n    grow_map = torch.load(path)\n\n    tree_size = grow_map[\"size\"]\n    idx_lists = grow_map[\"roots\"]\n    branch_lists = grow_map['branches']\n    draft_step = len(grow_map[\"roots\"])\n    sampling_callables = {}\n    sample_gather_indices = {}\n    for i in range(draft_step - 1):\n        idx_len = len(idx_lists[i])\n        num_samples = max(branch_lists[i])\n        sampling_callables[i] = cuda_graph_for_sampling_argmax(\n            max_length=args.M, idx_len=idx_len, num_samples=num_samples,\n            temperature=args.T, tree_size=tree_size)  \n    for i in range(draft_step - 1):\n        ith_gather_list = []\n        max_num_samples = max(branch_lists[i])\n        for j, branch in enumerate(branch_lists[i]):\n            branch_index = torch.arange(branch, device=\"cuda:0\", dtype=torch.long)\n            branch_index = branch_index + j * max_num_samples\n            ith_gather_list.append(branch_index)\n        ith_gather_list = torch.cat(ith_gather_list)\n        sample_gather_indices[i] = ith_gather_list\n    \n\n    \n\n\n\n\n\n\naccelerator = Accelerator()\ndataloader = accelerator.prepare(dataloader)\n\n#warm up functions:\n\nif args.Mode == 'benchmark':\n    simulation_greedy_with_tree_fast_benchmark(target_model=target_model, draft_model=draft_model, dataloader=dataloader, T=args.T, top_p=args.P, budget=args.B, draft_top_p=args.DP, w=args.W, negative=args.negative, decay=args.decay, static=args.static, \n                                               max_length=args.M, residual_graph = residual_graph, grow_map = grow_map, sampling_callables=sampling_callables, sample_gather_indices = sample_gather_indices)\nelif args.Mode == 'baseline':\n    simulation_baseline(target_model=target_model, dataloader=dataloader, T=args.T, top_p=args.P)\nelif args.Mode == 'greedy':\n    simulation_greedy_with_tree_fast(target_model=target_model, draft_model=draft_model, dataloader=dataloader, T=args.T, top_p=args.P, budget=args.B, draft_top_p=args.DP, w=args.W, negative=args.negative, decay=args.decay, static=args.static, \n                                     max_length=args.M, residual_graph = residual_graph, grow_map = grow_map, sampling_callables=sampling_callables, sample_gather_indices = sample_gather_indices)"
        }
    ]
}