{
    "sourceFile": "data_converter.py",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 4,
            "patches": [
                {
                    "date": 1712783819242,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                },
                {
                    "date": 1712789015131,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -44,8 +44,9 @@\n     def tokenize_function(examples):\n             return tokenizer(examples[\"text\"], return_tensors='pt',max_length=seq_len,padding=True,truncation=True)\n     dataset = dataset.map(tokenize_function, batched=True, remove_columns=['text'])\n     dataset.set_format(type='torch', columns=['input_ids', 'attention_mask'])\n+    dataset.save_to_disk(\"/home/zhuominc/SpeculativeDecoding/data/openwebtext_eval_long\")\n     return dataset\n \n def convert_cnn_dataset(tokenizer, seq_len = 256):\n     dataset = load_dataset(\"cnn_dailymail\", \"1.0.0\", split=\"test[0:2000]\")\n"
                },
                {
                    "date": 1712789050954,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -44,9 +44,9 @@\n     def tokenize_function(examples):\n             return tokenizer(examples[\"text\"], return_tensors='pt',max_length=seq_len,padding=True,truncation=True)\n     dataset = dataset.map(tokenize_function, batched=True, remove_columns=['text'])\n     dataset.set_format(type='torch', columns=['input_ids', 'attention_mask'])\n-    dataset.save_to_disk(\"/home/zhuominc/SpeculativeDecoding/data/openwebtext_eval_long\")\n+    dataset.save_to_disk(\"/home/zhuominc/Sequoia_mingxiaohuo/dataset/wikipedia\")\n     return dataset\n \n def convert_cnn_dataset(tokenizer, seq_len = 256):\n     dataset = load_dataset(\"cnn_dailymail\", \"1.0.0\", split=\"test[0:2000]\")\n"
                },
                {
                    "date": 1712789064599,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -53,8 +53,9 @@\n     def tokenize_function(examples):\n             return tokenizer(examples[\"article\"], return_tensors='pt',max_length=seq_len,padding=True,truncation=True)\n     dataset = dataset.map(tokenize_function, batched=True, remove_columns=['article'])\n     dataset.set_format(type='torch', columns=['input_ids', 'attention_mask'])\n+    dataset.save_to_disk(\"/home/zhuominc/Sequoia_mingxiaohuo/dataset/wikipedia\")\n     return dataset\n \n def convert_yahoo_dataset(tokenizer, seq_len = 256):\n     dataset = load_dataset(\"yahoo_answers_topics\", split=\"train\")\n"
                },
                {
                    "date": 1712789074996,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -53,9 +53,9 @@\n     def tokenize_function(examples):\n             return tokenizer(examples[\"article\"], return_tensors='pt',max_length=seq_len,padding=True,truncation=True)\n     dataset = dataset.map(tokenize_function, batched=True, remove_columns=['article'])\n     dataset.set_format(type='torch', columns=['input_ids', 'attention_mask'])\n-    dataset.save_to_disk(\"/home/zhuominc/Sequoia_mingxiaohuo/dataset/wikipedia\")\n+    dataset.save_to_disk(\"/home/zhuominc/Sequoia_mingxiaohuo/dataset/cnn\")\n     return dataset\n \n def convert_yahoo_dataset(tokenizer, seq_len = 256):\n     dataset = load_dataset(\"yahoo_answers_topics\", split=\"train\")\n"
                }
            ],
            "date": 1712783819242,
            "name": "Commit-0",
            "content": "import os\n#os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\nimport argparse\nimport time\n\nfrom tqdm.auto import tqdm\n\nimport torch\nimport transformers\nfrom transformers.models.opt.modeling_opt import OPTAttention, OPTDecoderLayer, OPTForCausalLM\nfrom transformers import GPT2Tokenizer, LlamaForCausalLM, LlamaModel, LlamaTokenizer\nfrom transformers import (\n    CONFIG_MAPPING,\n    MODEL_MAPPING,\n    AutoConfig,\n    AutoModelForSeq2SeqLM,\n    AutoTokenizer,\n    DataCollatorForSeq2Seq,\n    MBartTokenizer,\n    MBartTokenizerFast,\n    SchedulerType,\n    default_data_collator,\n    get_scheduler,\n)\nimport torch\nfrom accelerate import Accelerator\nfrom accelerate.logging import get_logger\nfrom accelerate.utils import set_seed\n\nimport pandas as pd\n\nfrom datasets import load_dataset, concatenate_datasets\n\nfrom transformers.utils import check_min_version, get_full_repo_name, send_example_telemetry\nfrom transformers.utils.versions import require_version\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\n# Will error if the minimal version of Transformers is not installed. Remove at your own risks.\ncheck_min_version(\"4.28.0.dev0\")\n\nlogger = get_logger(__name__)\nrequire_version(\"datasets>=1.8.0\", \"To fix: pip install -r examples/pytorch/translation/requirements.txt\")\ndef convert_wiki_dataset(tokenizer, seq_len = 256):\n    dataset = load_dataset(\"wikimedia/wikipedia\", \"20231101.en\", split=\"train[0:2000]\")\n    def tokenize_function(examples):\n            return tokenizer(examples[\"text\"], return_tensors='pt',max_length=seq_len,padding=True,truncation=True)\n    dataset = dataset.map(tokenize_function, batched=True, remove_columns=['text'])\n    dataset.set_format(type='torch', columns=['input_ids', 'attention_mask'])\n    return dataset\n\ndef convert_cnn_dataset(tokenizer, seq_len = 256):\n    dataset = load_dataset(\"cnn_dailymail\", \"1.0.0\", split=\"test[0:2000]\")\n    def tokenize_function(examples):\n            return tokenizer(examples[\"article\"], return_tensors='pt',max_length=seq_len,padding=True,truncation=True)\n    dataset = dataset.map(tokenize_function, batched=True, remove_columns=['article'])\n    dataset.set_format(type='torch', columns=['input_ids', 'attention_mask'])\n    return dataset\n\ndef convert_yahoo_dataset(tokenizer, seq_len = 256):\n    dataset = load_dataset(\"yahoo_answers_topics\", split=\"train\")\n    def tokenize_function(examples):\n            tk = tokenizer(examples[\"best_answer\"], return_tensors='pt',max_length=seq_len,padding=True,truncation=True)\n            ret = {\n                'input_ids': tk['input_ids'],\n                'attention_mask': tk['attention_mask'],\n                'labels': examples['topic']\n            }\n            return ret\n    dataset = dataset.map(tokenize_function, batched=True, remove_columns=['id', 'topic', 'question_title', 'question_content', 'best_answer'])\n    dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n    return dataset\ndef convert_yahoo_dataset_eval(tokenizer, seq_len = 256):\n    dataset = load_dataset(\"yahoo_answers_topics\", split=\"test\")\n    def tokenize_function(examples):\n            tk = tokenizer(examples[\"best_answer\"], return_tensors='pt',max_length=seq_len,padding=True,truncation=True)\n            ret = {\n                'input_ids': tk['input_ids'],\n                'attention_mask': tk['attention_mask'],\n                'labels': examples['topic']\n            }\n            return ret\n    dataset = dataset.map(tokenize_function, batched=True, remove_columns=['id', 'topic', 'question_title', 'question_content', 'best_answer'])\n    dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n    return dataset\ndef convert_ag_dataset(tokenizer, seq_len = 256):\n    dataset = load_dataset(\"ag_news\", split=\"train\")\n    def tokenize_function(examples):\n            tk = tokenizer(examples[\"text\"], return_tensors='pt',max_length=seq_len,padding=True,truncation=True)\n            ret = {\n                'input_ids': tk['input_ids'],\n                'attention_mask': tk['attention_mask'],\n                'labels': examples['label']\n            }\n            return ret\n    dataset = dataset.map(tokenize_function, batched=True, remove_columns=['text', 'label'])\n\n    dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n    return dataset\n\ndef convert_ag_dataset_eval(tokenizer, seq_len = 256):\n    dataset = load_dataset(\"ag_news\", split=\"test\")\n\n    def tokenize_function(examples):\n            tk = tokenizer(examples[\"text\"], return_tensors='pt',max_length=seq_len,padding=True,truncation=True)\n            ret = {\n                'input_ids': tk['input_ids'],\n                'attention_mask': tk['attention_mask'],\n                'labels': examples['label']\n            }\n            return ret\n    dataset = dataset.map(tokenize_function, batched=True, remove_columns=['text'])\n\n    dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n    return dataset\ndef convert_openweb_dataset_eval(tokenizer, seq_len = 256):\n    dataset = load_dataset(\"openwebtext\", split=\"train[8010000:]\")\n    def tokenize_function(examples):\n            return tokenizer(examples[\"text\"], return_tensors='pt',max_length=seq_len,padding=True,truncation=True)\n    dataset = dataset.map(tokenize_function, batched=True, remove_columns=['text'])\n    dataset.set_format(type='torch', columns=['input_ids', 'attention_mask'])\n    dataset.save_to_disk(\"/home/zhuominc/SpeculativeDecoding/data/openwebtext_eval_long\")\n    return dataset\n\ndef convert_c4_dataset(tokenizer, seq_len = 256):\n    dataset = load_dataset(\"c4\", \"realnewslike\", split=\"train\")\n    def tokenize_function(examples):\n            return tokenizer(examples[\"text\"], return_tensors='pt',max_length=seq_len,padding=True,truncation=True)\n    dataset = dataset.map(tokenize_function, batched=True, remove_columns=['text', 'timestamp', 'url'])\n    dataset.set_format(type='torch', columns=['input_ids', 'attention_mask'])\n    #dataset.save_to_disk(\"/home/zhuominc/SpeculativeDecoding/data/c4_train\")\n    return dataset\n\ndef convert_c4_dataset_eval(tokenizer, seq_len = 256):\n    dataset = load_dataset(\"c4\", \"realnewslike\", split=\"validation[0:100]\")\n    def tokenize_function(examples):\n            return tokenizer(examples[\"text\"], return_tensors='pt',max_length=seq_len,padding=True,truncation=True)\n    dataset = dataset.map(tokenize_function, batched=True, remove_columns=['text', 'timestamp', 'url'])\n    dataset.set_format(type='torch', columns=['input_ids', 'attention_mask'])\n    #dataset.save_to_disk(\"/home/zhuominc/SpeculativeDecoding/data/validation\")\n    return dataset\ndef convert_dataset(tokenizer, file_path):\n    dataset = load_dataset(\"json\", data_files=file_path, split=\"train\")\n    def tokenize_function(examples):\n            input_ids = torch.Tensor(examples['input_ids'])\n            labels = input_ids.clone()\n            if tokenizer.pad_token_id is not None:\n                 labels[labels == tokenizer.pad_token_id] = -100\n            ret = {\n                \"input_ids\": input_ids,\n                \"labels\": labels\n            }\n            return ret\n    dataset = dataset.map(tokenize_function, batched=True, remove_columns=['input_tokens'])\n    dataset.set_format(type='torch', columns=['input_ids', \"labels\"])\n    return dataset\n\ndef convert_dataset_truncate(tokenizer, file_path, truncate_length = 100):\n    dataset = load_dataset(\"json\", data_files=file_path, split=\"train\")\n    def tokenize_function(examples):\n            src = tokenizer(examples['input'],return_tensors='pt',max_length=128,padding=True,truncation=True)\n            dst = tokenizer(examples['output'],return_tensors='pt',max_length=129,padding=True,truncation=True)\n            src_input_ids = src['input_ids']\n            dst_input_ids = dst['input_ids']\n            src_attn_mask = src['attention_mask']\n            dst_attn_mask = dst['attention_mask']\n            input_ids = torch.cat([src_input_ids, dst_input_ids[...,1:]], dim=1).long()\n            attn_mask = torch.cat([src_attn_mask, dst_attn_mask[...,1:]], dim=1).long()\n            labels = input_ids.clone()\n            if tokenizer.pad_token_id is not None:\n                 labels[labels == tokenizer.pad_token_id] = -100\n            ret = {\n                \"input_ids\": input_ids[..., truncate_length:],\n                \"attention_mask\": attn_mask[..., truncate_length:],\n                \"labels\": labels[..., truncate_length:]\n            }\n            return ret\n    dataset = dataset.map(tokenize_function, batched=True, remove_columns=['input', 'output'])\n    dataset.set_format(type='torch', columns=['input_ids', \"attention_mask\",\"labels\"])\n    return dataset\ndef generate_dataset(tokenizer, seq_len = 256):\n    \n    dataset1 = load_dataset(\"c4\", \"realnewslike\", split=\"validation\")\n    dataset2 = load_dataset(\"c4\", \"en\", split=\"validation[:20000]\")\n    dataset3 = load_dataset(\"openwebtext\", split=\"train[8000000:]\")\n    dataset1.remove_columns(['timestamp', 'url'])\n    dataset2.remove_columns(['timestamp', 'url'])\n    dataset_large = concatenate_datasets([dataset1, dataset2, dataset3])\n    def tokenize_function(examples):\n            return tokenizer(examples[\"text\"], return_tensors='pt',max_length=seq_len,padding=True,truncation=True)\n    dataset_large = dataset_large.map(tokenize_function, batched=True, remove_columns=['text', 'timestamp', 'url'])\n    dataset_large.set_format(type='torch', columns=['input_ids', 'attention_mask'])\n    dataset_large.save_to_disk(\"/home/zhuominc/SpeculativeDecoding/data/dataset_large_valid\")\n    return dataset_large\ndef main():\n    tokenizer = LlamaTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\", use_fast=False)\n    tokenizer.pad_token = tokenizer.eos_token\n    generate_dataset(tokenizer=tokenizer)\n\nif __name__ == '__main__':\n    main()"
        }
    ]
}