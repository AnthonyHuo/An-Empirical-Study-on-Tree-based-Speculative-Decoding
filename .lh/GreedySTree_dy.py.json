{
    "sourceFile": "GreedySTree_dy.py",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 51,
            "patches": [
                {
                    "date": 1712783736372,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                },
                {
                    "date": 1712784339001,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -205,9 +205,9 @@\n         \n         tree_mask.masked_fill_(tree_mask > 0, torch.finfo(self.dtype).min)\n         self.initialize(attn_mask, sequence, new_tokens_buffer, parents_buffer, position_ids, None)\n         self.set_prefix(prefix=prefix)\n-        self.tree_size = self.grow_map[\"size\"]\n+        self.tree_size = 128\n         self.tree_mask = tree_mask\n \n         self.full_attn_mask[self.max_length - self.tree_size + 1: self.max_length, self.max_length - self.tree_size + 1: self.max_length] = tree_mask[1:, 1:]\n         self.root = TreeNode()\n"
                },
                {
                    "date": 1712784372175,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -287,9 +287,9 @@\n         attn_mask = self.attn_mask[self.num_nodes: self.num_nodes+127]\n         tree_mask = generate_mask_for_pruned_tree(self.root)\n         tree_mask = (tree_mask == 0).type(self.dtype)\n         tree_mask.masked_fill_(tree_mask > 0, torch.finfo(self.dtype).min)\n-        attn_mask[0:127 , self.num_nodes:self.num_nodes+127] = tree_mask\n+        attn_mask[0:self.tree_size-1 , self.num_nodes:self.num_nodes+self.tree_size-1] = tree_mask\n         attn_mask = attn_mask[None, None, :, :]\n \n         draft_model_outputs = self.draft_model_engine.inference(\n             input_ids = self.tokens[self.num_nodes: self.num_nodes+127].unsqueeze(0),\n"
                },
                {
                    "date": 1712784383990,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -283,9 +283,9 @@\n         # self.num_nodes = self.num_nodes + 128\n         # nodes = [node for node in get_all_nodes(self.root)]\n         \n         position_ids = torch.tensor(get_all_nodes_depth(self.root)[1:])+self.num_nodes-1\n-        attn_mask = self.attn_mask[self.num_nodes: self.num_nodes+127]\n+        attn_mask = self.attn_mask[self.num_nodes: self.num_nodes+self.tree_size-1]\n         tree_mask = generate_mask_for_pruned_tree(self.root)\n         tree_mask = (tree_mask == 0).type(self.dtype)\n         tree_mask.masked_fill_(tree_mask > 0, torch.finfo(self.dtype).min)\n         attn_mask[0:self.tree_size-1 , self.num_nodes:self.num_nodes+self.tree_size-1] = tree_mask\n"
                },
                {
                    "date": 1712784420868,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -271,9 +271,9 @@\n                 t1 = time.time()\n         if grow_step == 0:\n            sampling_logits = self.draft_logits[0].unsqueeze(0)\n         else:\n-           sampling_logits = self.draft_logits[1:128]\n+           sampling_logits = self.draft_logits[1:self.tree_size]\n         nodes = [node for node in get_all_nodes(self.root)]\n         self.update_tree_with_logits(sampling_logits, nodes, grow_step)\n         self.tokens[self.num_nodes: self.num_nodes + 127] = torch.tensor(get_all_nodes_value(self.root)[1:])\n         if benchmark:\n"
                },
                {
                    "date": 1712784430108,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -274,9 +274,9 @@\n         else:\n            sampling_logits = self.draft_logits[1:self.tree_size]\n         nodes = [node for node in get_all_nodes(self.root)]\n         self.update_tree_with_logits(sampling_logits, nodes, grow_step)\n-        self.tokens[self.num_nodes: self.num_nodes + 127] = torch.tensor(get_all_nodes_value(self.root)[1:])\n+        self.tokens[self.num_nodes: self.num_nodes + self.tree_size-1] = torch.tensor(get_all_nodes_value(self.root)[1:])\n         if benchmark:\n                     torch.cuda.synchronize()\n                     t2 = time.time()\n                     x1 += (t2 - t1)\n"
                },
                {
                    "date": 1712784462666,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -291,9 +291,9 @@\n         attn_mask[0:self.tree_size-1 , self.num_nodes:self.num_nodes+self.tree_size-1] = tree_mask\n         attn_mask = attn_mask[None, None, :, :]\n \n         draft_model_outputs = self.draft_model_engine.inference(\n-            input_ids = self.tokens[self.num_nodes: self.num_nodes+127].unsqueeze(0),\n+            input_ids = self.tokens[self.num_nodes: self.num_nodes+self.tree_size-1].unsqueeze(0),\n             position_ids = position_ids.unsqueeze(0),\n             attn_mask = attn_mask,\n             storage_ids=self.storage_ids[self.num_nodes: self.num_nodes+127]\n             \n"
                },
                {
                    "date": 1712784468374,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -294,9 +294,9 @@\n         draft_model_outputs = self.draft_model_engine.inference(\n             input_ids = self.tokens[self.num_nodes: self.num_nodes+self.tree_size-1].unsqueeze(0),\n             position_ids = position_ids.unsqueeze(0),\n             attn_mask = attn_mask,\n-            storage_ids=self.storage_ids[self.num_nodes: self.num_nodes+127]\n+            storage_ids=self.storage_ids[self.num_nodes: self.num_nodes+self.tree_size-1]\n             \n         )\n         self.draft_kv_len = self.num_nodes+127\n         self.draft_logits[1:128] = draft_model_outputs[0][-127:]\n"
                },
                {
                    "date": 1712784480138,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -297,9 +297,9 @@\n             attn_mask = attn_mask,\n             storage_ids=self.storage_ids[self.num_nodes: self.num_nodes+self.tree_size-1]\n             \n         )\n-        self.draft_kv_len = self.num_nodes+127\n+        self.draft_kv_len = self.num_nodes+self.tree_size-1\n         self.draft_logits[1:128] = draft_model_outputs[0][-127:]\n         if benchmark:\n                     torch.cuda.synchronize()\n                     t3 = time.time()\n"
                },
                {
                    "date": 1712784496111,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -298,9 +298,9 @@\n             storage_ids=self.storage_ids[self.num_nodes: self.num_nodes+self.tree_size-1]\n             \n         )\n         self.draft_kv_len = self.num_nodes+self.tree_size-1\n-        self.draft_logits[1:128] = draft_model_outputs[0][-127:]\n+        self.draft_logits[1:self.tree_size] = draft_model_outputs[0][-127:]\n         if benchmark:\n                     torch.cuda.synchronize()\n                     t3 = time.time()\n                     x2 += (t3 - t2)\n"
                },
                {
                    "date": 1712784513987,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -298,9 +298,9 @@\n             storage_ids=self.storage_ids[self.num_nodes: self.num_nodes+self.tree_size-1]\n             \n         )\n         self.draft_kv_len = self.num_nodes+self.tree_size-1\n-        self.draft_logits[1:self.tree_size] = draft_model_outputs[0][-127:]\n+        self.draft_logits[1:self.tree_size] = draft_model_outputs[0][1-self.tree_size:]\n         if benchmark:\n                     torch.cuda.synchronize()\n                     t3 = time.time()\n                     x2 += (t3 - t2)\n"
                },
                {
                    "date": 1712784549901,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -413,9 +413,9 @@\n                         sample_time += t1\n                         compute_time += t2   \n                 else:\n                         self.collective_grow_dynamic(grow_step=i)\n-        self.num_nodes = self.num_nodes+127\n+        self.num_nodes = self.num_nodes+self.tree_size-1\n         self.Successors = generate_successors_list(self.root)\n         if benchmark:\n             return sample_time, compute_time\n         else:\n"
                },
                {
                    "date": 1712784589700,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -256,9 +256,9 @@\n                     child = TreeNode(logit=logit.item(), value=value.item(), parent=parent_node, depth=parent_node.depth + 1, updated=False)\n                     parent_node.add_child(child)\n                 parent_node.updated = True\n             i+=1\n-        prune_tree(self.root, keep=127)  # Assuming root is defined and accessible\n+        prune_tree(self.root, keep=self.tree_size-1)  # Assuming root is defined and accessible\n     \n     @torch.inference_mode()\n     def collective_grow_dynamic(self, benchmark=False, grow_step = None):\n         \n"
                },
                {
                    "date": 1712785048001,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -130,36 +130,36 @@\n     # Ensure the root is always kept\n     keep_nodes.add(root)\n \n     bfs_prune_tree(root, keep_nodes)\n-def generate_mask_for_pruned_tree(root):\n-    # Step 1: Collect all nodes using BFS, assuming the tree is already pruned to 128 nodes.\n-    all_nodes = get_all_nodes(root)\n-    all_nodes = all_nodes[1:]\n-    node_to_index = {node: index for index, node in enumerate(all_nodes)}\n-    # Step 2: Create a mapping of node to index.\n-    # node_to_index = {node: index for index, node in enumerate(all_nodes)}\n+# def generate_mask_for_pruned_tree(root):\n+#     # Step 1: Collect all nodes using BFS, assuming the tree is already pruned to 128 nodes.\n+#     all_nodes = get_all_nodes(root)\n+#     all_nodes = all_nodes[1:]\n+#     node_to_index = {node: index for index, node in enumerate(all_nodes)}\n+#     # Step 2: Create a mapping of node to index.\n+#     # node_to_index = {node: index for index, node in enumerate(all_nodes)}\n     \n-    # Step 3: Initialize the mask with zeros.\n-    mask = [[0 for _ in range(127)] for _ in range(127)]\n+#     # Step 3: Initialize the mask with zeros.\n+#     mask = [[0 for _ in range(127)] for _ in range(127)]\n     \n-    # Step 4: Populate the mask.\n-    for node in all_nodes:\n-        current_index = node_to_index[node]\n-        # Set the node's own position in the mask to 1.\n-        mask[current_index][current_index] = 1\n+#     # Step 4: Populate the mask.\n+#     for node in all_nodes:\n+#         current_index = node_to_index[node]\n+#         # Set the node's own position in the mask to 1.\n+#         mask[current_index][current_index] = 1\n         \n-        # Traverse up to the root to set ancestor relations.\n-        current_node = node.parent\n-        while current_node.value is not None:\n-            parent_index = node_to_index[current_node]\n-            mask[current_index][parent_index] = 1  # Mark the ancestor's relation to this node.\n-            current_node = current_node.parent\n+#         # Traverse up to the root to set ancestor relations.\n+#         current_node = node.parent\n+#         while current_node.value is not None:\n+#             parent_index = node_to_index[current_node]\n+#             mask[current_index][parent_index] = 1  # Mark the ancestor's relation to this node.\n+#             current_node = current_node.parent\n     \n-    # Convert mask to a tensor\n-    mask_tensor = torch.tensor(mask, dtype=torch.int)\n+#     # Convert mask to a tensor\n+#     mask_tensor = torch.tensor(mask, dtype=torch.int)\n     \n-    return mask_tensor\n+#     return mask_tensor\n class GreedySTree(Tree):\n     def __init__(self, \n                  #draft_model :LlamaForCausalLM_Attn, \n                  draft_model_engine :GraphInferenceEngine,\n"
                },
                {
                    "date": 1712785066617,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -238,8 +238,36 @@\n         self.draft_kv_len = self.num_nodes\n         \n         self.target_kv_len = target_kv_len\n         self.seq_to_use = list(range(self.max_length))\n+    def generate_mask_for_pruned_tree(root):\n+        # Step 1: Collect all nodes using BFS, assuming the tree is already pruned to 128 nodes.\n+        all_nodes = get_all_nodes(root)\n+        all_nodes = all_nodes[1:]\n+        node_to_index = {node: index for index, node in enumerate(all_nodes)}\n+        # Step 2: Create a mapping of node to index.\n+        # node_to_index = {node: index for index, node in enumerate(all_nodes)}\n+        \n+        # Step 3: Initialize the mask with zeros.\n+        mask = [[0 for _ in range(127)] for _ in range(127)]\n+        \n+        # Step 4: Populate the mask.\n+        for node in all_nodes:\n+            current_index = node_to_index[node]\n+            # Set the node's own position in the mask to 1.\n+            mask[current_index][current_index] = 1\n+            \n+            # Traverse up to the root to set ancestor relations.\n+            current_node = node.parent\n+            while current_node.value is not None:\n+                parent_index = node_to_index[current_node]\n+                mask[current_index][parent_index] = 1  # Mark the ancestor's relation to this node.\n+                current_node = current_node.parent\n+        \n+        # Convert mask to a tensor\n+        mask_tensor = torch.tensor(mask, dtype=torch.int)\n+        \n+        return mask_tensor\n     def update_tree_with_logits(self, logits, parent_nodes, grow_step):\n         \"\"\"Update the tree dynamically with new tokens and logits.\"\"\"\n         if grow_step !=0:\n             new_tokens_values, new_tokens_set = logits.topk(k=31)\n"
                },
                {
                    "date": 1712785090104,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -238,9 +238,9 @@\n         self.draft_kv_len = self.num_nodes\n         \n         self.target_kv_len = target_kv_len\n         self.seq_to_use = list(range(self.max_length))\n-    def generate_mask_for_pruned_tree(root):\n+    def generate_mask_for_pruned_tree(self,root):\n         # Step 1: Collect all nodes using BFS, assuming the tree is already pruned to 128 nodes.\n         all_nodes = get_all_nodes(root)\n         all_nodes = all_nodes[1:]\n         node_to_index = {node: index for index, node in enumerate(all_nodes)}\n"
                },
                {
                    "date": 1712785235810,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -312,9 +312,9 @@\n         # nodes = [node for node in get_all_nodes(self.root)]\n         \n         position_ids = torch.tensor(get_all_nodes_depth(self.root)[1:])+self.num_nodes-1\n         attn_mask = self.attn_mask[self.num_nodes: self.num_nodes+self.tree_size-1]\n-        tree_mask = generate_mask_for_pruned_tree(self.root)\n+        tree_mask = self.generate_mask_for_pruned_tree(self.root)\n         tree_mask = (tree_mask == 0).type(self.dtype)\n         tree_mask.masked_fill_(tree_mask > 0, torch.finfo(self.dtype).min)\n         attn_mask[0:self.tree_size-1 , self.num_nodes:self.num_nodes+self.tree_size-1] = tree_mask\n         attn_mask = attn_mask[None, None, :, :]\n"
                },
                {
                    "date": 1712785279099,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -247,9 +247,9 @@\n         # Step 2: Create a mapping of node to index.\n         # node_to_index = {node: index for index, node in enumerate(all_nodes)}\n         \n         # Step 3: Initialize the mask with zeros.\n-        mask = [[0 for _ in range(127)] for _ in range(127)]\n+        mask = [[0 for _ in range(self.tree_size-1)] for _ in range(self.tree_size-1)]\n         \n         # Step 4: Populate the mask.\n         for node in all_nodes:\n             current_index = node_to_index[node]\n"
                },
                {
                    "date": 1712785538026,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -205,9 +205,9 @@\n         \n         tree_mask.masked_fill_(tree_mask > 0, torch.finfo(self.dtype).min)\n         self.initialize(attn_mask, sequence, new_tokens_buffer, parents_buffer, position_ids, None)\n         self.set_prefix(prefix=prefix)\n-        self.tree_size = 128\n+        self.tree_size = 64\n         self.tree_mask = tree_mask\n \n         self.full_attn_mask[self.max_length - self.tree_size + 1: self.max_length, self.max_length - self.tree_size + 1: self.max_length] = tree_mask[1:, 1:]\n         self.root = TreeNode()\n"
                },
                {
                    "date": 1712785699731,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -208,9 +208,9 @@\n         self.set_prefix(prefix=prefix)\n         self.tree_size = 64\n         self.tree_mask = tree_mask\n \n-        self.full_attn_mask[self.max_length - self.tree_size + 1: self.max_length, self.max_length - self.tree_size + 1: self.max_length] = tree_mask[1:, 1:]\n+        # self.full_attn_mask[self.max_length - self.tree_size + 1: self.max_length, self.max_length - self.tree_size + 1: self.max_length] = tree_mask[1:, 1:]\n         self.root = TreeNode()\n \n         total_nodes = len(prefix) + self.tree_size - 1\n         self.attn_mask = self.full_attn_mask[self.max_length - total_nodes: 2 * self.max_length - total_nodes, self.max_length - total_nodes: 2 * self.max_length - total_nodes]\n"
                },
                {
                    "date": 1712785761871,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -216,9 +216,9 @@\n         self.attn_mask = self.full_attn_mask[self.max_length - total_nodes: 2 * self.max_length - total_nodes, self.max_length - total_nodes: 2 * self.max_length - total_nodes]\n         self.ground_truth_len = len(prefix)\n         \n         \n-        self.position_ids[len(prefix) : len(prefix) + self.tree_size - 1] = (self.grow_map[\"depth\"][1:].to(self.device) + len(prefix) - 1)\n+        # self.position_ids[len(prefix) : len(prefix) + self.tree_size - 1] = (self.grow_map[\"depth\"][1:].to(self.device) + len(prefix) - 1)\n         self.storage_ids = torch.arange(self.max_length).to(self.device)\n         self.depth = self.grow_map[\"depth\"][1:].to(self.device)\n         \n         self.draft_logits = torch.zeros((self.max_length, vocab_size), dtype=self.dtype).to(self.device)\n"
                },
                {
                    "date": 1712785898897,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -10,9 +10,8 @@\n import deepspeed\n from Engine import GraphInferenceEngine, GraphInferenceEngineTG\n from torch.profiler import profile, record_function, ProfilerActivity\n from utils import get_sampling_logits, make_tree_attention_mask, select_kv, ChildrenAccept, get_residual, cat_kv, _make_causal_mask\n-from data_converter import convert_wiki_dataset, convert_cnn_dataset, convert_c4_dataset_eval\n class TreeNode:\n     def __init__(self, logit=0, value=None, parent=None, depth=0, updated=False):\n         self.logit = logit  # Logit value for this node\n         self.value = value  # The actual token value this node represents\n@@ -130,36 +129,36 @@\n     # Ensure the root is always kept\n     keep_nodes.add(root)\n \n     bfs_prune_tree(root, keep_nodes)\n-# def generate_mask_for_pruned_tree(root):\n-#     # Step 1: Collect all nodes using BFS, assuming the tree is already pruned to 128 nodes.\n-#     all_nodes = get_all_nodes(root)\n-#     all_nodes = all_nodes[1:]\n-#     node_to_index = {node: index for index, node in enumerate(all_nodes)}\n-#     # Step 2: Create a mapping of node to index.\n-#     # node_to_index = {node: index for index, node in enumerate(all_nodes)}\n+def generate_mask_for_pruned_tree(root):\n+    # Step 1: Collect all nodes using BFS, assuming the tree is already pruned to 128 nodes.\n+    all_nodes = get_all_nodes(root)\n+    all_nodes = all_nodes[1:]\n+    node_to_index = {node: index for index, node in enumerate(all_nodes)}\n+    # Step 2: Create a mapping of node to index.\n+    # node_to_index = {node: index for index, node in enumerate(all_nodes)}\n     \n-#     # Step 3: Initialize the mask with zeros.\n-#     mask = [[0 for _ in range(127)] for _ in range(127)]\n+    # Step 3: Initialize the mask with zeros.\n+    mask = [[0 for _ in range(127)] for _ in range(127)]\n     \n-#     # Step 4: Populate the mask.\n-#     for node in all_nodes:\n-#         current_index = node_to_index[node]\n-#         # Set the node's own position in the mask to 1.\n-#         mask[current_index][current_index] = 1\n+    # Step 4: Populate the mask.\n+    for node in all_nodes:\n+        current_index = node_to_index[node]\n+        # Set the node's own position in the mask to 1.\n+        mask[current_index][current_index] = 1\n         \n-#         # Traverse up to the root to set ancestor relations.\n-#         current_node = node.parent\n-#         while current_node.value is not None:\n-#             parent_index = node_to_index[current_node]\n-#             mask[current_index][parent_index] = 1  # Mark the ancestor's relation to this node.\n-#             current_node = current_node.parent\n+        # Traverse up to the root to set ancestor relations.\n+        current_node = node.parent\n+        while current_node.value is not None:\n+            parent_index = node_to_index[current_node]\n+            mask[current_index][parent_index] = 1  # Mark the ancestor's relation to this node.\n+            current_node = current_node.parent\n     \n-#     # Convert mask to a tensor\n-#     mask_tensor = torch.tensor(mask, dtype=torch.int)\n+    # Convert mask to a tensor\n+    mask_tensor = torch.tensor(mask, dtype=torch.int)\n     \n-#     return mask_tensor\n+    return mask_tensor\n class GreedySTree(Tree):\n     def __init__(self, \n                  #draft_model :LlamaForCausalLM_Attn, \n                  draft_model_engine :GraphInferenceEngine,\n@@ -205,20 +204,20 @@\n         \n         tree_mask.masked_fill_(tree_mask > 0, torch.finfo(self.dtype).min)\n         self.initialize(attn_mask, sequence, new_tokens_buffer, parents_buffer, position_ids, None)\n         self.set_prefix(prefix=prefix)\n-        self.tree_size = 64\n+        self.tree_size = self.grow_map[\"size\"]\n         self.tree_mask = tree_mask\n \n-        # self.full_attn_mask[self.max_length - self.tree_size + 1: self.max_length, self.max_length - self.tree_size + 1: self.max_length] = tree_mask[1:, 1:]\n+        self.full_attn_mask[self.max_length - self.tree_size + 1: self.max_length, self.max_length - self.tree_size + 1: self.max_length] = tree_mask[1:, 1:]\n         self.root = TreeNode()\n \n         total_nodes = len(prefix) + self.tree_size - 1\n         self.attn_mask = self.full_attn_mask[self.max_length - total_nodes: 2 * self.max_length - total_nodes, self.max_length - total_nodes: 2 * self.max_length - total_nodes]\n         self.ground_truth_len = len(prefix)\n         \n         \n-        # self.position_ids[len(prefix) : len(prefix) + self.tree_size - 1] = (self.grow_map[\"depth\"][1:].to(self.device) + len(prefix) - 1)\n+        self.position_ids[len(prefix) : len(prefix) + self.tree_size - 1] = (self.grow_map[\"depth\"][1:].to(self.device) + len(prefix) - 1)\n         self.storage_ids = torch.arange(self.max_length).to(self.device)\n         self.depth = self.grow_map[\"depth\"][1:].to(self.device)\n         \n         self.draft_logits = torch.zeros((self.max_length, vocab_size), dtype=self.dtype).to(self.device)\n@@ -238,36 +237,8 @@\n         self.draft_kv_len = self.num_nodes\n         \n         self.target_kv_len = target_kv_len\n         self.seq_to_use = list(range(self.max_length))\n-    def generate_mask_for_pruned_tree(self,root):\n-        # Step 1: Collect all nodes using BFS, assuming the tree is already pruned to 128 nodes.\n-        all_nodes = get_all_nodes(root)\n-        all_nodes = all_nodes[1:]\n-        node_to_index = {node: index for index, node in enumerate(all_nodes)}\n-        # Step 2: Create a mapping of node to index.\n-        # node_to_index = {node: index for index, node in enumerate(all_nodes)}\n-        \n-        # Step 3: Initialize the mask with zeros.\n-        mask = [[0 for _ in range(self.tree_size-1)] for _ in range(self.tree_size-1)]\n-        \n-        # Step 4: Populate the mask.\n-        for node in all_nodes:\n-            current_index = node_to_index[node]\n-            # Set the node's own position in the mask to 1.\n-            mask[current_index][current_index] = 1\n-            \n-            # Traverse up to the root to set ancestor relations.\n-            current_node = node.parent\n-            while current_node.value is not None:\n-                parent_index = node_to_index[current_node]\n-                mask[current_index][parent_index] = 1  # Mark the ancestor's relation to this node.\n-                current_node = current_node.parent\n-        \n-        # Convert mask to a tensor\n-        mask_tensor = torch.tensor(mask, dtype=torch.int)\n-        \n-        return mask_tensor\n     def update_tree_with_logits(self, logits, parent_nodes, grow_step):\n         \"\"\"Update the tree dynamically with new tokens and logits.\"\"\"\n         if grow_step !=0:\n             new_tokens_values, new_tokens_set = logits.topk(k=31)\n@@ -284,9 +255,9 @@\n                     child = TreeNode(logit=logit.item(), value=value.item(), parent=parent_node, depth=parent_node.depth + 1, updated=False)\n                     parent_node.add_child(child)\n                 parent_node.updated = True\n             i+=1\n-        prune_tree(self.root, keep=self.tree_size-1)  # Assuming root is defined and accessible\n+        prune_tree(self.root, keep=127)  # Assuming root is defined and accessible\n     \n     @torch.inference_mode()\n     def collective_grow_dynamic(self, benchmark=False, grow_step = None):\n         \n@@ -299,36 +270,36 @@\n                 t1 = time.time()\n         if grow_step == 0:\n            sampling_logits = self.draft_logits[0].unsqueeze(0)\n         else:\n-           sampling_logits = self.draft_logits[1:self.tree_size]\n+           sampling_logits = self.draft_logits[1:128]\n         nodes = [node for node in get_all_nodes(self.root)]\n         self.update_tree_with_logits(sampling_logits, nodes, grow_step)\n-        self.tokens[self.num_nodes: self.num_nodes + self.tree_size-1] = torch.tensor(get_all_nodes_value(self.root)[1:])\n+        self.tokens[self.num_nodes: self.num_nodes + 127] = torch.tensor(get_all_nodes_value(self.root)[1:])\n         if benchmark:\n                     torch.cuda.synchronize()\n                     t2 = time.time()\n                     x1 += (t2 - t1)\n         # self.num_nodes = self.num_nodes + 128\n         # nodes = [node for node in get_all_nodes(self.root)]\n         \n         position_ids = torch.tensor(get_all_nodes_depth(self.root)[1:])+self.num_nodes-1\n-        attn_mask = self.attn_mask[self.num_nodes: self.num_nodes+self.tree_size-1]\n-        tree_mask = self.generate_mask_for_pruned_tree(self.root)\n+        attn_mask = self.attn_mask[self.num_nodes: self.num_nodes+127]\n+        tree_mask = generate_mask_for_pruned_tree(self.root)\n         tree_mask = (tree_mask == 0).type(self.dtype)\n         tree_mask.masked_fill_(tree_mask > 0, torch.finfo(self.dtype).min)\n-        attn_mask[0:self.tree_size-1 , self.num_nodes:self.num_nodes+self.tree_size-1] = tree_mask\n+        attn_mask[0:127 , self.num_nodes:self.num_nodes+127] = tree_mask\n         attn_mask = attn_mask[None, None, :, :]\n \n         draft_model_outputs = self.draft_model_engine.inference(\n-            input_ids = self.tokens[self.num_nodes: self.num_nodes+self.tree_size-1].unsqueeze(0),\n+            input_ids = self.tokens[self.num_nodes: self.num_nodes+127].unsqueeze(0),\n             position_ids = position_ids.unsqueeze(0),\n             attn_mask = attn_mask,\n-            storage_ids=self.storage_ids[self.num_nodes: self.num_nodes+self.tree_size-1]\n+            storage_ids=self.storage_ids[self.num_nodes: self.num_nodes+127]\n             \n         )\n-        self.draft_kv_len = self.num_nodes+self.tree_size-1\n-        self.draft_logits[1:self.tree_size] = draft_model_outputs[0][1-self.tree_size:]\n+        self.draft_kv_len = self.num_nodes+127\n+        self.draft_logits[1:128] = draft_model_outputs[0][-127:]\n         if benchmark:\n                     torch.cuda.synchronize()\n                     t3 = time.time()\n                     x2 += (t3 - t2)\n@@ -441,9 +412,9 @@\n                         sample_time += t1\n                         compute_time += t2   \n                 else:\n                         self.collective_grow_dynamic(grow_step=i)\n-        self.num_nodes = self.num_nodes+self.tree_size-1\n+        self.num_nodes = self.num_nodes+127\n         self.Successors = generate_successors_list(self.root)\n         if benchmark:\n             return sample_time, compute_time\n         else:\n"
                },
                {
                    "date": 1712789426567,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -193,9 +193,9 @@\n         self.residual_graph = residual_graph\n         self.grow_map = grow_map\n         self.sampling_callables = sampling_callables\n         self.sample_gather_indices = sample_gather_indices\n-        self.draft_step = 6\n+        self.draft_step = 7\n         self.grow_map_roots_gpu = []\n         for x in self.grow_map[\"roots\"]:\n              self.grow_map_roots_gpu.append(torch.Tensor(x).to(self.device).long())\n         self.Successors = self.grow_map[\"Successors\"]\n"
                },
                {
                    "date": 1712792364827,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -305,26 +305,49 @@\n                     x2 += (t3 - t2)\n         if benchmark:\n             return x1, x2\n         return None\n+    # @torch.inference_mode()\n+    # def accept_step(self, parent_id :int) ->ChildrenAccept:\n+    #     logits_id = parent_id - (self.ground_truth_len - 1)\n+        \n+    #     target_token = self.target_token[logits_id]\n+    #     children = self.Successors[logits_id]\n+    #     if len(children) == 0:\n+    #         return -1\n+        \n+    #     for pos in children:\n+\n+    #         token = self.tokens[pos + (self.ground_truth_len - 1)]\n+    #         if token == target_token:\n+    #             return pos + (self.ground_truth_len - 1)\n+        \n+    #     return -1\n     @torch.inference_mode()\n     def accept_step(self, parent_id :int) ->ChildrenAccept:\n         logits_id = parent_id - (self.ground_truth_len - 1)\n+        p = self.target_logits[logits_id]\n         \n-        target_token = self.target_token[logits_id]\n+        draft_logits = self.draft_logits[logits_id]\n         children = self.Successors[logits_id]\n         if len(children) == 0:\n-            return -1\n+            return ChildrenAccept(accept_mark=2, residual=p)\n         \n-        for pos in children:\n+        for idx, pos in enumerate(children):\n \n             token = self.tokens[pos + (self.ground_truth_len - 1)]\n-            if token == target_token:\n-                return pos + (self.ground_truth_len - 1)\n+            q = softmax(draft_logits / self.temperature, dim=-1)\n+            r = self.r[pos + (self.ground_truth_len - 1)]\n+            if p[token] >= r * q[token]:\n+                return ChildrenAccept(accept_mark=0, token=token, position=pos + (self.ground_truth_len - 1), successor_order=idx)\n+            else:\n+                p = get_residual(p, q)\n+                draft_logits[token] = -torch.inf\n         \n-        return -1\n+        return ChildrenAccept(accept_mark=1, residual=p)\n \n \n+\n         \n     @torch.inference_mode()\n     def verify(self, benchmark = False):\n         new_node_num = (self.num_nodes - self.ground_truth_len + 1)\n"
                },
                {
                    "date": 1712793071692,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -305,49 +305,26 @@\n                     x2 += (t3 - t2)\n         if benchmark:\n             return x1, x2\n         return None\n-    # @torch.inference_mode()\n-    # def accept_step(self, parent_id :int) ->ChildrenAccept:\n-    #     logits_id = parent_id - (self.ground_truth_len - 1)\n-        \n-    #     target_token = self.target_token[logits_id]\n-    #     children = self.Successors[logits_id]\n-    #     if len(children) == 0:\n-    #         return -1\n-        \n-    #     for pos in children:\n-\n-    #         token = self.tokens[pos + (self.ground_truth_len - 1)]\n-    #         if token == target_token:\n-    #             return pos + (self.ground_truth_len - 1)\n-        \n-    #     return -1\n     @torch.inference_mode()\n     def accept_step(self, parent_id :int) ->ChildrenAccept:\n         logits_id = parent_id - (self.ground_truth_len - 1)\n-        p = self.target_logits[logits_id]\n         \n-        draft_logits = self.draft_logits[logits_id]\n+        target_token = self.target_token[logits_id]\n         children = self.Successors[logits_id]\n         if len(children) == 0:\n-            return ChildrenAccept(accept_mark=2, residual=p)\n+            return -1\n         \n-        for idx, pos in enumerate(children):\n+        for pos in children:\n \n             token = self.tokens[pos + (self.ground_truth_len - 1)]\n-            q = softmax(draft_logits / self.temperature, dim=-1)\n-            r = self.r[pos + (self.ground_truth_len - 1)]\n-            if p[token] >= r * q[token]:\n-                return ChildrenAccept(accept_mark=0, token=token, position=pos + (self.ground_truth_len - 1), successor_order=idx)\n-            else:\n-                p = get_residual(p, q)\n-                draft_logits[token] = -torch.inf\n+            if token == target_token:\n+                return pos + (self.ground_truth_len - 1)\n         \n-        return ChildrenAccept(accept_mark=1, residual=p)\n+        return -1\n \n \n-\n         \n     @torch.inference_mode()\n     def verify(self, benchmark = False):\n         new_node_num = (self.num_nodes - self.ground_truth_len + 1)\n"
                },
                {
                    "date": 1712793780364,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -129,36 +129,36 @@\n     # Ensure the root is always kept\n     keep_nodes.add(root)\n \n     bfs_prune_tree(root, keep_nodes)\n-def generate_mask_for_pruned_tree(root):\n-    # Step 1: Collect all nodes using BFS, assuming the tree is already pruned to 128 nodes.\n-    all_nodes = get_all_nodes(root)\n-    all_nodes = all_nodes[1:]\n-    node_to_index = {node: index for index, node in enumerate(all_nodes)}\n-    # Step 2: Create a mapping of node to index.\n-    # node_to_index = {node: index for index, node in enumerate(all_nodes)}\n+# def generate_mask_for_pruned_tree(root):\n+#     # Step 1: Collect all nodes using BFS, assuming the tree is already pruned to 128 nodes.\n+#     all_nodes = get_all_nodes(root)\n+#     all_nodes = all_nodes[1:]\n+#     node_to_index = {node: index for index, node in enumerate(all_nodes)}\n+#     # Step 2: Create a mapping of node to index.\n+#     # node_to_index = {node: index for index, node in enumerate(all_nodes)}\n     \n-    # Step 3: Initialize the mask with zeros.\n-    mask = [[0 for _ in range(127)] for _ in range(127)]\n+#     # Step 3: Initialize the mask with zeros.\n+#     mask = [[0 for _ in range(127)] for _ in range(127)]\n     \n-    # Step 4: Populate the mask.\n-    for node in all_nodes:\n-        current_index = node_to_index[node]\n-        # Set the node's own position in the mask to 1.\n-        mask[current_index][current_index] = 1\n+#     # Step 4: Populate the mask.\n+#     for node in all_nodes:\n+#         current_index = node_to_index[node]\n+#         # Set the node's own position in the mask to 1.\n+#         mask[current_index][current_index] = 1\n         \n-        # Traverse up to the root to set ancestor relations.\n-        current_node = node.parent\n-        while current_node.value is not None:\n-            parent_index = node_to_index[current_node]\n-            mask[current_index][parent_index] = 1  # Mark the ancestor's relation to this node.\n-            current_node = current_node.parent\n+#         # Traverse up to the root to set ancestor relations.\n+#         current_node = node.parent\n+#         while current_node.value is not None:\n+#             parent_index = node_to_index[current_node]\n+#             mask[current_index][parent_index] = 1  # Mark the ancestor's relation to this node.\n+#             current_node = current_node.parent\n     \n-    # Convert mask to a tensor\n-    mask_tensor = torch.tensor(mask, dtype=torch.int)\n+#     # Convert mask to a tensor\n+#     mask_tensor = torch.tensor(mask, dtype=torch.int)\n     \n-    return mask_tensor\n+#     return mask_tensor\n class GreedySTree(Tree):\n     def __init__(self, \n                  #draft_model :LlamaForCausalLM_Attn, \n                  draft_model_engine :GraphInferenceEngine,\n"
                },
                {
                    "date": 1712793794987,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -237,8 +237,36 @@\n         self.draft_kv_len = self.num_nodes\n         \n         self.target_kv_len = target_kv_len\n         self.seq_to_use = list(range(self.max_length))\n+    def generate_mask_for_pruned_tree(root):\n+        # Step 1: Collect all nodes using BFS, assuming the tree is already pruned to 128 nodes.\n+        all_nodes = get_all_nodes(root)\n+        all_nodes = all_nodes[1:]\n+        node_to_index = {node: index for index, node in enumerate(all_nodes)}\n+        # Step 2: Create a mapping of node to index.\n+        # node_to_index = {node: index for index, node in enumerate(all_nodes)}\n+        \n+        # Step 3: Initialize the mask with zeros.\n+        mask = [[0 for _ in range(127)] for _ in range(127)]\n+        \n+        # Step 4: Populate the mask.\n+        for node in all_nodes:\n+            current_index = node_to_index[node]\n+            # Set the node's own position in the mask to 1.\n+            mask[current_index][current_index] = 1\n+            \n+            # Traverse up to the root to set ancestor relations.\n+            current_node = node.parent\n+            while current_node.value is not None:\n+                parent_index = node_to_index[current_node]\n+                mask[current_index][parent_index] = 1  # Mark the ancestor's relation to this node.\n+                current_node = current_node.parent\n+        \n+        # Convert mask to a tensor\n+        mask_tensor = torch.tensor(mask, dtype=torch.int)\n+        \n+        return mask_tensor\n     def update_tree_with_logits(self, logits, parent_nodes, grow_step):\n         \"\"\"Update the tree dynamically with new tokens and logits.\"\"\"\n         if grow_step !=0:\n             new_tokens_values, new_tokens_set = logits.topk(k=31)\n"
                },
                {
                    "date": 1712793800930,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -237,9 +237,9 @@\n         self.draft_kv_len = self.num_nodes\n         \n         self.target_kv_len = target_kv_len\n         self.seq_to_use = list(range(self.max_length))\n-    def generate_mask_for_pruned_tree(root):\n+    def generate_mask_for_pruned_tree(self,root):\n         # Step 1: Collect all nodes using BFS, assuming the tree is already pruned to 128 nodes.\n         all_nodes = get_all_nodes(root)\n         all_nodes = all_nodes[1:]\n         node_to_index = {node: index for index, node in enumerate(all_nodes)}\n"
                },
                {
                    "date": 1712793842562,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -311,9 +311,9 @@\n         # nodes = [node for node in get_all_nodes(self.root)]\n         \n         position_ids = torch.tensor(get_all_nodes_depth(self.root)[1:])+self.num_nodes-1\n         attn_mask = self.attn_mask[self.num_nodes: self.num_nodes+127]\n-        tree_mask = generate_mask_for_pruned_tree(self.root)\n+        tree_mask = self.generate_mask_for_pruned_tree(self.root)\n         tree_mask = (tree_mask == 0).type(self.dtype)\n         tree_mask.masked_fill_(tree_mask > 0, torch.finfo(self.dtype).min)\n         attn_mask[0:127 , self.num_nodes:self.num_nodes+127] = tree_mask\n         attn_mask = attn_mask[None, None, :, :]\n"
                },
                {
                    "date": 1712794132736,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -450,11 +450,11 @@\n     def prepare_for_next_iter(self, accept_list: list[int], valid_tokens :torch.LongTensor):\n         if len(accept_list) + 1 > self.max_target_seq:\n               return \n         # self.tokens[:len(valid_tokens)] = valid_tokens\n-        self.position_ids[:len(accept_list)] =  self.position_ids[accept_list]\n-        self.position_ids[len(accept_list)] = len(accept_list) \n-        self.position_ids[len(valid_tokens) : len(valid_tokens) + self.tree_size - 1] = (self.depth + len(valid_tokens) - 1)\n+        # self.position_ids[:len(accept_list)] =  self.position_ids[accept_list]\n+        # self.position_ids[len(accept_list)] = len(accept_list) \n+        # self.position_ids[len(valid_tokens) : len(valid_tokens) + self.tree_size - 1] = (self.depth + len(valid_tokens) - 1)\n         self.ground_truth_len = len(valid_tokens)\n         self.num_nodes = len(valid_tokens)\n \n         total_nodes = len(valid_tokens) + self.tree_size - 1\n"
                },
                {
                    "date": 1712794373070,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -449,12 +449,12 @@\n             return None\n     def prepare_for_next_iter(self, accept_list: list[int], valid_tokens :torch.LongTensor):\n         if len(accept_list) + 1 > self.max_target_seq:\n               return \n-        # self.tokens[:len(valid_tokens)] = valid_tokens\n-        # self.position_ids[:len(accept_list)] =  self.position_ids[accept_list]\n-        # self.position_ids[len(accept_list)] = len(accept_list) \n-        # self.position_ids[len(valid_tokens) : len(valid_tokens) + self.tree_size - 1] = (self.depth + len(valid_tokens) - 1)\n+        self.tokens[:len(valid_tokens)] = valid_tokens\n+        self.position_ids[:len(accept_list)] =  self.position_ids[accept_list]\n+        self.position_ids[len(accept_list)] = len(accept_list) \n+        self.position_ids[len(valid_tokens) : len(valid_tokens) + self.tree_size - 1] = (self.depth + len(valid_tokens) - 1)\n         self.ground_truth_len = len(valid_tokens)\n         self.num_nodes = len(valid_tokens)\n \n         total_nodes = len(valid_tokens) + self.tree_size - 1\n"
                },
                {
                    "date": 1712794482987,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -450,11 +450,11 @@\n     def prepare_for_next_iter(self, accept_list: list[int], valid_tokens :torch.LongTensor):\n         if len(accept_list) + 1 > self.max_target_seq:\n               return \n         self.tokens[:len(valid_tokens)] = valid_tokens\n-        self.position_ids[:len(accept_list)] =  self.position_ids[accept_list]\n-        self.position_ids[len(accept_list)] = len(accept_list) \n-        self.position_ids[len(valid_tokens) : len(valid_tokens) + self.tree_size - 1] = (self.depth + len(valid_tokens) - 1)\n+        # self.position_ids[:len(accept_list)] =  self.position_ids[accept_list]\n+        # self.position_ids[len(accept_list)] = len(accept_list) \n+        # self.position_ids[len(valid_tokens) : len(valid_tokens) + self.tree_size - 1] = (self.depth + len(valid_tokens) - 1)\n         self.ground_truth_len = len(valid_tokens)\n         self.num_nodes = len(valid_tokens)\n \n         total_nodes = len(valid_tokens) + self.tree_size - 1\n"
                },
                {
                    "date": 1712794704310,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -450,11 +450,11 @@\n     def prepare_for_next_iter(self, accept_list: list[int], valid_tokens :torch.LongTensor):\n         if len(accept_list) + 1 > self.max_target_seq:\n               return \n         self.tokens[:len(valid_tokens)] = valid_tokens\n-        # self.position_ids[:len(accept_list)] =  self.position_ids[accept_list]\n-        # self.position_ids[len(accept_list)] = len(accept_list) \n-        # self.position_ids[len(valid_tokens) : len(valid_tokens) + self.tree_size - 1] = (self.depth + len(valid_tokens) - 1)\n+        self.position_ids[:len(accept_list)] =  self.position_ids[accept_list]\n+        self.position_ids[len(accept_list)] = len(accept_list) \n+        self.position_ids[len(valid_tokens) : len(valid_tokens) + self.tree_size - 1] = (self.depth + len(valid_tokens) - 1)\n         self.ground_truth_len = len(valid_tokens)\n         self.num_nodes = len(valid_tokens)\n \n         total_nodes = len(valid_tokens) + self.tree_size - 1\n"
                },
                {
                    "date": 1713447983724,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -193,9 +193,9 @@\n         self.residual_graph = residual_graph\n         self.grow_map = grow_map\n         self.sampling_callables = sampling_callables\n         self.sample_gather_indices = sample_gather_indices\n-        self.draft_step = 7\n+        self.draft_step = 6\n         self.grow_map_roots_gpu = []\n         for x in self.grow_map[\"roots\"]:\n              self.grow_map_roots_gpu.append(torch.Tensor(x).to(self.device).long())\n         self.Successors = self.grow_map[\"Successors\"]\n@@ -325,16 +325,16 @@\n             storage_ids=self.storage_ids[self.num_nodes: self.num_nodes+127]\n             \n         )\n         self.draft_kv_len = self.num_nodes+127\n-        self.draft_logits[1:128] = draft_model_outputs[0][-127:]\n+        self.draft_logits[1:64] = draft_model_outputs[0][-127:]\n         if benchmark:\n                     torch.cuda.synchronize()\n                     t3 = time.time()\n                     x2 += (t3 - t2)\n         if benchmark:\n             return x1, x2\n-        return None\n+        return position_ids, tree_mask\n     @torch.inference_mode()\n     def accept_step(self, parent_id :int) ->ChildrenAccept:\n         logits_id = parent_id - (self.ground_truth_len - 1)\n         \n@@ -359,12 +359,14 @@\n         if self.target_kv_len == 0:\n             start_pos = 0\n             end_pos = self.num_nodes\n             attn_mask = self.attn_mask[start_pos: end_pos, :end_pos]\n+            attn_mask[end_pos-127:end_pos,end_pos-127:end_pos] = self.draft_tree_mask\n             attn_mask = attn_mask[None, None, :, :]\n             if benchmark:\n                 torch.cuda.synchronize()\n                 t1 = time.time()\n+            self.position_ids[end_pos-127:end_pos] = self.draft_postion_ids\n             target_model_outputs = self.target_model_engine.inference(input_ids = self.tokens[start_pos : end_pos].unsqueeze(0), \n                                     position_ids = self.position_ids[start_pos : end_pos].unsqueeze(0), attn_mask = attn_mask, \n                                     storage_ids=self.storage_ids[start_pos : end_pos])\n             if benchmark:\n@@ -375,12 +377,14 @@\n         else:\n             start_pos = self.target_kv_len\n             end_pos = self.num_nodes\n             attn_mask = self.attn_mask[start_pos: end_pos, :end_pos]\n+            attn_mask[1:,end_pos-127:end_pos] = self.draft_tree_mask\n             attn_mask = attn_mask[None, None, :, :]\n             if benchmark:\n                 torch.cuda.synchronize()\n                 t1 = time.time()\n+            self.position_ids[start_pos+1 : end_pos] = self.draft_postion_ids\n             target_model_outputs = self.target_model_engine.inference(input_ids = self.tokens[start_pos : end_pos].unsqueeze(0), \n                                         position_ids =self.position_ids[start_pos : end_pos].unsqueeze(0), attn_mask = attn_mask,\n                                         storage_ids=self.storage_ids[start_pos : end_pos])\n             if benchmark:\n@@ -389,9 +393,9 @@\n             self.target_logits :torch.FloatTensor = target_model_outputs[0][-(new_node_num):]\n         \n         self.target_logits = get_sampling_logits(logits=self.target_logits, top_p=self.top_p, T=self.temperature, replicate=False)\n         self.target_logits = softmax(self.target_logits / self.temperature, dim=-1)\n-        #self.target_token = self.target_logits.argmax(dim=-1)\n+        # self.target_token = self.target_logits.argmax(dim=-1)\n         self.target_token = self.target_logits.multinomial(num_samples=1)\n         accept_list = self.seq_to_use[:self.ground_truth_len]\n         \n         terminal = False\n@@ -439,9 +443,11 @@\n                         _, t1, t2 = self.collective_grow_static(self.grow_map_roots_gpu[i], self.grow_map['branches'][i], benchmark=benchmark, grow_step=i)\n                         sample_time += t1\n                         compute_time += t2   \n                 else:\n-                        self.collective_grow_dynamic(grow_step=i)\n+                        position_ids, tree_mask = self.collective_grow_dynamic(grow_step=i)\n+        self.draft_postion_ids = position_ids\n+        self.draft_tree_mask = tree_mask\n         self.num_nodes = self.num_nodes+127\n         self.Successors = generate_successors_list(self.root)\n         if benchmark:\n             return sample_time, compute_time\n"
                },
                {
                    "date": 1713447992661,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -298,9 +298,9 @@\n                 t1 = time.time()\n         if grow_step == 0:\n            sampling_logits = self.draft_logits[0].unsqueeze(0)\n         else:\n-           sampling_logits = self.draft_logits[1:128]\n+           sampling_logits = self.draft_logits[1:64]\n         nodes = [node for node in get_all_nodes(self.root)]\n         self.update_tree_with_logits(sampling_logits, nodes, grow_step)\n         self.tokens[self.num_nodes: self.num_nodes + 127] = torch.tensor(get_all_nodes_value(self.root)[1:])\n         if benchmark:\n"
                },
                {
                    "date": 1713448017761,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -106,9 +106,9 @@\n         all_nodes.append(current_node.depth)\n         queue.extend(current_node.children)  # Children are added to the end of the queue\n     return all_nodes\n \n-def prune_tree(root, keep=127):\n+def prune_tree(root, keep=63):\n     \"\"\"Prune the tree to keep up to 'keep' nodes based on unique cumulative logits, prioritizing uniqueness.\"\"\"\n     all_nodes = get_all_nodes(root)\n     # Compute cumulative logits excluding the root\n     all_nodes_with_logits = [(node, node.compute_cumulative_logit()) for node in all_nodes if node.parent is not None]\n"
                },
                {
                    "date": 1713448025520,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -246,9 +246,9 @@\n         # Step 2: Create a mapping of node to index.\n         # node_to_index = {node: index for index, node in enumerate(all_nodes)}\n         \n         # Step 3: Initialize the mask with zeros.\n-        mask = [[0 for _ in range(127)] for _ in range(127)]\n+        mask = [[0 for _ in range(63)] for _ in range(63)]\n         \n         # Step 4: Populate the mask.\n         for node in all_nodes:\n             current_index = node_to_index[node]\n"
                },
                {
                    "date": 1713448037596,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -283,9 +283,9 @@\n                     child = TreeNode(logit=logit.item(), value=value.item(), parent=parent_node, depth=parent_node.depth + 1, updated=False)\n                     parent_node.add_child(child)\n                 parent_node.updated = True\n             i+=1\n-        prune_tree(self.root, keep=127)  # Assuming root is defined and accessible\n+        prune_tree(self.root, keep=63)  # Assuming root is defined and accessible\n     \n     @torch.inference_mode()\n     def collective_grow_dynamic(self, benchmark=False, grow_step = None):\n         \n"
                },
                {
                    "date": 1713448046117,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -301,9 +301,9 @@\n         else:\n            sampling_logits = self.draft_logits[1:64]\n         nodes = [node for node in get_all_nodes(self.root)]\n         self.update_tree_with_logits(sampling_logits, nodes, grow_step)\n-        self.tokens[self.num_nodes: self.num_nodes + 127] = torch.tensor(get_all_nodes_value(self.root)[1:])\n+        self.tokens[self.num_nodes: self.num_nodes + 63] = torch.tensor(get_all_nodes_value(self.root)[1:])\n         if benchmark:\n                     torch.cuda.synchronize()\n                     t2 = time.time()\n                     x1 += (t2 - t1)\n"
                },
                {
                    "date": 1713448078589,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -310,9 +310,9 @@\n         # self.num_nodes = self.num_nodes + 128\n         # nodes = [node for node in get_all_nodes(self.root)]\n         \n         position_ids = torch.tensor(get_all_nodes_depth(self.root)[1:])+self.num_nodes-1\n-        attn_mask = self.attn_mask[self.num_nodes: self.num_nodes+127]\n+        attn_mask = self.attn_mask[self.num_nodes: self.num_nodes+63]\n         tree_mask = self.generate_mask_for_pruned_tree(self.root)\n         tree_mask = (tree_mask == 0).type(self.dtype)\n         tree_mask.masked_fill_(tree_mask > 0, torch.finfo(self.dtype).min)\n         attn_mask[0:127 , self.num_nodes:self.num_nodes+127] = tree_mask\n"
                },
                {
                    "date": 1713448102446,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -314,9 +314,9 @@\n         attn_mask = self.attn_mask[self.num_nodes: self.num_nodes+63]\n         tree_mask = self.generate_mask_for_pruned_tree(self.root)\n         tree_mask = (tree_mask == 0).type(self.dtype)\n         tree_mask.masked_fill_(tree_mask > 0, torch.finfo(self.dtype).min)\n-        attn_mask[0:127 , self.num_nodes:self.num_nodes+127] = tree_mask\n+        attn_mask[0:127 , self.num_nodes:self.num_nodes+63] = tree_mask\n         attn_mask = attn_mask[None, None, :, :]\n \n         draft_model_outputs = self.draft_model_engine.inference(\n             input_ids = self.tokens[self.num_nodes: self.num_nodes+127].unsqueeze(0),\n"
                },
                {
                    "date": 1713448297666,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -318,16 +318,16 @@\n         attn_mask[0:127 , self.num_nodes:self.num_nodes+63] = tree_mask\n         attn_mask = attn_mask[None, None, :, :]\n \n         draft_model_outputs = self.draft_model_engine.inference(\n-            input_ids = self.tokens[self.num_nodes: self.num_nodes+127].unsqueeze(0),\n+            input_ids = self.tokens[self.num_nodes: self.num_nodes+63].unsqueeze(0),\n             position_ids = position_ids.unsqueeze(0),\n             attn_mask = attn_mask,\n-            storage_ids=self.storage_ids[self.num_nodes: self.num_nodes+127]\n+            storage_ids=self.storage_ids[self.num_nodes: self.num_nodes+63]\n             \n         )\n-        self.draft_kv_len = self.num_nodes+127\n-        self.draft_logits[1:64] = draft_model_outputs[0][-127:]\n+        self.draft_kv_len = self.num_nodes+63\n+        self.draft_logits[1:64] = draft_model_outputs[0][-63:]\n         if benchmark:\n                     torch.cuda.synchronize()\n                     t3 = time.time()\n                     x2 += (t3 - t2)\n"
                },
                {
                    "date": 1713448325302,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -359,9 +359,9 @@\n         if self.target_kv_len == 0:\n             start_pos = 0\n             end_pos = self.num_nodes\n             attn_mask = self.attn_mask[start_pos: end_pos, :end_pos]\n-            attn_mask[end_pos-127:end_pos,end_pos-127:end_pos] = self.draft_tree_mask\n+            attn_mask[end_pos-63:end_pos,end_pos-63:end_pos] = self.draft_tree_mask\n             attn_mask = attn_mask[None, None, :, :]\n             if benchmark:\n                 torch.cuda.synchronize()\n                 t1 = time.time()\n"
                },
                {
                    "date": 1713448331281,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -364,9 +364,9 @@\n             attn_mask = attn_mask[None, None, :, :]\n             if benchmark:\n                 torch.cuda.synchronize()\n                 t1 = time.time()\n-            self.position_ids[end_pos-127:end_pos] = self.draft_postion_ids\n+            self.position_ids[end_pos-63:end_pos] = self.draft_postion_ids\n             target_model_outputs = self.target_model_engine.inference(input_ids = self.tokens[start_pos : end_pos].unsqueeze(0), \n                                     position_ids = self.position_ids[start_pos : end_pos].unsqueeze(0), attn_mask = attn_mask, \n                                     storage_ids=self.storage_ids[start_pos : end_pos])\n             if benchmark:\n"
                },
                {
                    "date": 1713448355689,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -377,9 +377,9 @@\n         else:\n             start_pos = self.target_kv_len\n             end_pos = self.num_nodes\n             attn_mask = self.attn_mask[start_pos: end_pos, :end_pos]\n-            attn_mask[1:,end_pos-127:end_pos] = self.draft_tree_mask\n+            attn_mask[1:,end_pos-63:end_pos] = self.draft_tree_mask\n             attn_mask = attn_mask[None, None, :, :]\n             if benchmark:\n                 torch.cuda.synchronize()\n                 t1 = time.time()\n"
                },
                {
                    "date": 1713448363529,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -446,9 +446,9 @@\n                 else:\n                         position_ids, tree_mask = self.collective_grow_dynamic(grow_step=i)\n         self.draft_postion_ids = position_ids\n         self.draft_tree_mask = tree_mask\n-        self.num_nodes = self.num_nodes+127\n+        self.num_nodes = self.num_nodes+63\n         self.Successors = generate_successors_list(self.root)\n         if benchmark:\n             return sample_time, compute_time\n         else:\n"
                },
                {
                    "date": 1713448921864,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -314,9 +314,9 @@\n         attn_mask = self.attn_mask[self.num_nodes: self.num_nodes+63]\n         tree_mask = self.generate_mask_for_pruned_tree(self.root)\n         tree_mask = (tree_mask == 0).type(self.dtype)\n         tree_mask.masked_fill_(tree_mask > 0, torch.finfo(self.dtype).min)\n-        attn_mask[0:127 , self.num_nodes:self.num_nodes+63] = tree_mask\n+        attn_mask[0:63 , self.num_nodes:self.num_nodes+63] = tree_mask\n         attn_mask = attn_mask[None, None, :, :]\n \n         draft_model_outputs = self.draft_model_engine.inference(\n             input_ids = self.tokens[self.num_nodes: self.num_nodes+63].unsqueeze(0),\n"
                },
                {
                    "date": 1713449114893,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -193,9 +193,9 @@\n         self.residual_graph = residual_graph\n         self.grow_map = grow_map\n         self.sampling_callables = sampling_callables\n         self.sample_gather_indices = sample_gather_indices\n-        self.draft_step = 6\n+        self.draft_step = 7\n         self.grow_map_roots_gpu = []\n         for x in self.grow_map[\"roots\"]:\n              self.grow_map_roots_gpu.append(torch.Tensor(x).to(self.device).long())\n         self.Successors = self.grow_map[\"Successors\"]\n"
                },
                {
                    "date": 1713449284770,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -391,12 +391,12 @@\n                 torch.cuda.synchronize()\n                 t2 = time.time()\n             self.target_logits :torch.FloatTensor = target_model_outputs[0][-(new_node_num):]\n         \n-        self.target_logits = get_sampling_logits(logits=self.target_logits, top_p=self.top_p, T=self.temperature, replicate=False)\n-        self.target_logits = softmax(self.target_logits / self.temperature, dim=-1)\n+        # self.target_logits = get_sampling_logits(logits=self.target_logits, top_p=self.top_p, T=self.temperature, replicate=False)\n+        # self.target_logits = softmax(self.target_logits / self.temperature, dim=-1)\n         # self.target_token = self.target_logits.argmax(dim=-1)\n-        self.target_token = self.target_logits.multinomial(num_samples=1)\n+        # self.target_token = self.target_logits.multinomial(num_samples=1)\n         accept_list = self.seq_to_use[:self.ground_truth_len]\n         \n         terminal = False\n         while True:\n"
                },
                {
                    "date": 1713450891655,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -193,9 +193,9 @@\n         self.residual_graph = residual_graph\n         self.grow_map = grow_map\n         self.sampling_callables = sampling_callables\n         self.sample_gather_indices = sample_gather_indices\n-        self.draft_step = 7\n+        self.draft_step = 6\n         self.grow_map_roots_gpu = []\n         for x in self.grow_map[\"roots\"]:\n              self.grow_map_roots_gpu.append(torch.Tensor(x).to(self.device).long())\n         self.Successors = self.grow_map[\"Successors\"]\n@@ -393,9 +393,9 @@\n             self.target_logits :torch.FloatTensor = target_model_outputs[0][-(new_node_num):]\n         \n         # self.target_logits = get_sampling_logits(logits=self.target_logits, top_p=self.top_p, T=self.temperature, replicate=False)\n         # self.target_logits = softmax(self.target_logits / self.temperature, dim=-1)\n-        # self.target_token = self.target_logits.argmax(dim=-1)\n+        self.target_token = self.target_logits.argmax(dim=-1)\n         # self.target_token = self.target_logits.multinomial(num_samples=1)\n         accept_list = self.seq_to_use[:self.ground_truth_len]\n         \n         terminal = False\n"
                },
                {
                    "date": 1713450941468,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -391,12 +391,12 @@\n                 torch.cuda.synchronize()\n                 t2 = time.time()\n             self.target_logits :torch.FloatTensor = target_model_outputs[0][-(new_node_num):]\n         \n-        # self.target_logits = get_sampling_logits(logits=self.target_logits, top_p=self.top_p, T=self.temperature, replicate=False)\n-        # self.target_logits = softmax(self.target_logits / self.temperature, dim=-1)\n-        self.target_token = self.target_logits.argmax(dim=-1)\n-        # self.target_token = self.target_logits.multinomial(num_samples=1)\n+        self.target_logits = get_sampling_logits(logits=self.target_logits, top_p=self.top_p, T=self.temperature, replicate=False)\n+        self.target_logits = softmax(self.target_logits / self.temperature, dim=-1)\n+        # self.target_token = self.target_logits.argmax(dim=-1)\n+        self.target_token = self.target_logits.multinomial(num_samples=1)\n         accept_list = self.seq_to_use[:self.ground_truth_len]\n         \n         terminal = False\n         while True:\n"
                },
                {
                    "date": 1713455319466,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -391,12 +391,12 @@\n                 torch.cuda.synchronize()\n                 t2 = time.time()\n             self.target_logits :torch.FloatTensor = target_model_outputs[0][-(new_node_num):]\n         \n-        self.target_logits = get_sampling_logits(logits=self.target_logits, top_p=self.top_p, T=self.temperature, replicate=False)\n-        self.target_logits = softmax(self.target_logits / self.temperature, dim=-1)\n-        # self.target_token = self.target_logits.argmax(dim=-1)\n-        self.target_token = self.target_logits.multinomial(num_samples=1)\n+        # self.target_logits = get_sampling_logits(logits=self.target_logits, top_p=self.top_p, T=self.temperature, replicate=False)\n+        # self.target_logits = softmax(self.target_logits / self.temperature, dim=-1)\n+        self.target_token = self.target_logits.argmax(dim=-1)\n+        # self.target_token = self.target_logits.multinomial(num_samples=1)\n         accept_list = self.seq_to_use[:self.ground_truth_len]\n         \n         terminal = False\n         while True:\n"
                }
            ],
            "date": 1712783736372,
            "name": "Commit-0",
            "content": "import torch\nfrom transformers import AutoModelForCausalLM\nfrom transformers import LlamaForCausalLM, LlamaTokenizer, DataCollatorForLanguageModeling, OPTForCausalLM, AutoTokenizer\nfrom Llama import LlamaModel_Attn, LlamaForCausalLM_Attn\nfrom torch.nn.functional import softmax\nfrom copy import deepcopy\nfrom tqdm import tqdm\nfrom Tree import Tree\nimport time\nimport deepspeed\nfrom Engine import GraphInferenceEngine, GraphInferenceEngineTG\nfrom torch.profiler import profile, record_function, ProfilerActivity\nfrom utils import get_sampling_logits, make_tree_attention_mask, select_kv, ChildrenAccept, get_residual, cat_kv, _make_causal_mask\nfrom data_converter import convert_wiki_dataset, convert_cnn_dataset, convert_c4_dataset_eval\nclass TreeNode:\n    def __init__(self, logit=0, value=None, parent=None, depth=0, updated=False):\n        self.logit = logit  # Logit value for this node\n        self.value = value  # The actual token value this node represents\n        self.parent = parent\n        self.children = []\n        self.depth = depth\n        self.updated = updated \n\n    def compute_cumulative_logit(self):\n        \"\"\"Compute the cumulative logit from this node up to the root.\"\"\"\n        # if self.updated:\n        #     # For nodes not updated in the current iteration, return only the node's logit\n        #     return self.logit\n        # else: \n        node = self\n        cumulative_logit = 0\n        weight = self.depth*(-0.09)+1\n        while node:\n            cumulative_logit += (node.logit*weight)\n            # cumulative_logit += node.logit\n            node = node.parent\n            weight+=0.09\n        return cumulative_logit\n\n    def add_child(self, child):\n        self.children.append(child)\ndef bfs_prune_tree(root, keep_nodes):\n    \"\"\"Prune the tree using a BFS approach, keeping only nodes in 'keep_nodes'.\n    If a parent is not in 'keep_nodes', it and all its children are automatically pruned.\"\"\"\n    queue = [root]\n    while queue:\n        current_node = queue.pop(0)\n        # Proceed only if the current node is in 'keep_nodes'\n        if current_node in keep_nodes:\n            # Filter children to keep only those in 'keep_nodes'\n            filtered_children = [child for child in current_node.children if child in keep_nodes]\n            current_node.children = filtered_children\n            # Add filtered children to the queue\n            queue.extend(filtered_children)\ndef flatten_tree(root):\n    \"\"\"Flatten the tree into a list using BFS and return the list.\"\"\"\n    all_nodes = []\n    queue = [root]\n    while queue:\n        current_node = queue.pop(0)\n        all_nodes.append(current_node)\n        queue.extend(current_node.children)\n    return all_nodes\n\ndef generate_successors_list(root):\n    \"\"\"Generate the 2D list of successors for all nodes.\"\"\"\n    all_nodes = flatten_tree(root)\n    node_to_index = {node: i for i, node in enumerate(all_nodes)}  # Create a node to index mapping\n\n    successors = [[] for _ in range(len(all_nodes))]  # Preparing the 2D list for all nodes\n\n    # Populate the successor list for each node\n    for node in all_nodes:\n        node_index = node_to_index[node]\n        for child in node.children:\n            child_index = node_to_index[child]\n            successors[node_index].append(child_index)\n\n    return successors\n\ndef get_all_nodes(root):\n    \"\"\"Get all nodes in the tree starting from root using BFS.\"\"\"\n    all_nodes = []\n    queue = [root]\n    while queue:\n        current_node = queue.pop(0)  # Change to queue operation\n        all_nodes.append(current_node)\n        queue.extend(current_node.children)  # Children are added to the end of the queue\n    return all_nodes\n\ndef get_all_nodes_value(root):\n    \"\"\"Get all nodes in the tree starting from root using BFS.\"\"\"\n    all_nodes = []\n    queue = [root]\n    while queue:\n        current_node = queue.pop(0)  # Change to queue operation\n        all_nodes.append(current_node.value)\n        queue.extend(current_node.children)  # Children are added to the end of the queue\n    return all_nodes\n\ndef get_all_nodes_depth(root):\n    \"\"\"Get all nodes in the tree starting from root using BFS.\"\"\"\n    all_nodes = []\n    queue = [root]\n    while queue:\n        current_node = queue.pop(0)  # Change to queue operation\n        all_nodes.append(current_node.depth)\n        queue.extend(current_node.children)  # Children are added to the end of the queue\n    return all_nodes\n\ndef prune_tree(root, keep=127):\n    \"\"\"Prune the tree to keep up to 'keep' nodes based on unique cumulative logits, prioritizing uniqueness.\"\"\"\n    all_nodes = get_all_nodes(root)\n    # Compute cumulative logits excluding the root\n    all_nodes_with_logits = [(node, node.compute_cumulative_logit()) for node in all_nodes if node.parent is not None]\n\n    # Sort nodes based on cumulative logits to prioritize higher values\n    sorted_nodes = sorted(all_nodes_with_logits, key=lambda x: x[1], reverse=True)[:keep]\n    keep_nodes = set(node[0] for node in sorted_nodes)\n    # unique_values_and_parents = set()\n    # keep_nodes = set()\n    # for node, _ in sorted_nodes:\n    #     value_parent_pair = (node.value, node.parent)\n    #     if value_parent_pair not in unique_values_and_parents:\n    #         unique_values_and_parents.add(value_parent_pair)\n    #         keep_nodes.add(node)\n    #     if len(keep_nodes) >= keep:\n    #         break\n\n    # Ensure the root is always kept\n    keep_nodes.add(root)\n\n    bfs_prune_tree(root, keep_nodes)\ndef generate_mask_for_pruned_tree(root):\n    # Step 1: Collect all nodes using BFS, assuming the tree is already pruned to 128 nodes.\n    all_nodes = get_all_nodes(root)\n    all_nodes = all_nodes[1:]\n    node_to_index = {node: index for index, node in enumerate(all_nodes)}\n    # Step 2: Create a mapping of node to index.\n    # node_to_index = {node: index for index, node in enumerate(all_nodes)}\n    \n    # Step 3: Initialize the mask with zeros.\n    mask = [[0 for _ in range(127)] for _ in range(127)]\n    \n    # Step 4: Populate the mask.\n    for node in all_nodes:\n        current_index = node_to_index[node]\n        # Set the node's own position in the mask to 1.\n        mask[current_index][current_index] = 1\n        \n        # Traverse up to the root to set ancestor relations.\n        current_node = node.parent\n        while current_node.value is not None:\n            parent_index = node_to_index[current_node]\n            mask[current_index][parent_index] = 1  # Mark the ancestor's relation to this node.\n            current_node = current_node.parent\n    \n    # Convert mask to a tensor\n    mask_tensor = torch.tensor(mask, dtype=torch.int)\n    \n    return mask_tensor\nclass GreedySTree(Tree):\n    def __init__(self, \n                 #draft_model :LlamaForCausalLM_Attn, \n                 draft_model_engine :GraphInferenceEngine,\n                 target_model_engine :GraphInferenceEngineTG,\n                 prefix :torch.LongTensor,\n                 temperature :float = 0.6,\n                 top_p: float = 0.9,\n                 draft_kv_len = 0,\n                 target_kv_len = 0,\n                 max_length = 256,\n                 device :str = 'cpu',\n                 max_target_seq = 256,\n                 vocab_size = 32000,\n                 grow_map = None,\n                 attn_mask = None, \n                 sequence = None, \n                 new_tokens_buffer = None, \n                 parents_buffer = None, \n                 position_ids = None,\n                 residual_graph = None,\n                 sampling_callables = None,\n                 sample_gather_indices = None) -> None:\n        super().__init__(device=device, max_length=max_length)\n        assert self.max_length == draft_model_engine.engine.max_length\n        self.max_target_seq = max_target_seq\n        #self.draft_model = draft_model.to(self.device).eval()\n        # p 0.9 t 0.01\n        self.draft_model_engine = draft_model_engine\n        self.target_model_engine = target_model_engine\n        self.temperature = temperature\n        self.top_p = top_p\n        self.residual_graph = residual_graph\n        self.grow_map = grow_map\n        self.sampling_callables = sampling_callables\n        self.sample_gather_indices = sample_gather_indices\n        self.draft_step = 6\n        self.grow_map_roots_gpu = []\n        for x in self.grow_map[\"roots\"]:\n             self.grow_map_roots_gpu.append(torch.Tensor(x).to(self.device).long())\n        self.Successors = self.grow_map[\"Successors\"]\n        tree_mask :torch.Tensor = self.grow_map[\"mask\"].to(self.device)\n        tree_mask = (tree_mask == 0).type(self.dtype)\n        \n        tree_mask.masked_fill_(tree_mask > 0, torch.finfo(self.dtype).min)\n        self.initialize(attn_mask, sequence, new_tokens_buffer, parents_buffer, position_ids, None)\n        self.set_prefix(prefix=prefix)\n        self.tree_size = self.grow_map[\"size\"]\n        self.tree_mask = tree_mask\n\n        self.full_attn_mask[self.max_length - self.tree_size + 1: self.max_length, self.max_length - self.tree_size + 1: self.max_length] = tree_mask[1:, 1:]\n        self.root = TreeNode()\n\n        total_nodes = len(prefix) + self.tree_size - 1\n        self.attn_mask = self.full_attn_mask[self.max_length - total_nodes: 2 * self.max_length - total_nodes, self.max_length - total_nodes: 2 * self.max_length - total_nodes]\n        self.ground_truth_len = len(prefix)\n        \n        \n        self.position_ids[len(prefix) : len(prefix) + self.tree_size - 1] = (self.grow_map[\"depth\"][1:].to(self.device) + len(prefix) - 1)\n        self.storage_ids = torch.arange(self.max_length).to(self.device)\n        self.depth = self.grow_map[\"depth\"][1:].to(self.device)\n        \n        self.draft_logits = torch.zeros((self.max_length, vocab_size), dtype=self.dtype).to(self.device)\n        if draft_kv_len == 0:\n            draft_model_outputs = self.draft_model_engine.inference(input_ids=self.tokens[:self.num_nodes].unsqueeze(0), \n                                storage_ids=self.storage_ids[:self.num_nodes], \n                                position_ids=self.position_ids[:self.num_nodes].unsqueeze(0),\n                                attn_mask=self.attn_mask[:self.num_nodes][None, None, :, :])\n            self.draft_logits[0] :torch.FloatTensor= draft_model_outputs[...,-1,:][0]\n        \n        else:\n            draft_model_outputs = self.draft_model_engine.inference(input_ids = self.tokens[draft_kv_len: self.num_nodes].unsqueeze(0), \n                                                    storage_ids=self.storage_ids[draft_kv_len: self.num_nodes],\n                                                    position_ids=self.position_ids[draft_kv_len: self.num_nodes].unsqueeze(0),\n                                                    attn_mask=self.attn_mask[draft_kv_len: self.num_nodes][None, None, :, :])\n            self.draft_logits[0] :torch.FloatTensor = draft_model_outputs[...,-1,:][0]\n        self.draft_kv_len = self.num_nodes\n        \n        self.target_kv_len = target_kv_len\n        self.seq_to_use = list(range(self.max_length))\n    def update_tree_with_logits(self, logits, parent_nodes, grow_step):\n        \"\"\"Update the tree dynamically with new tokens and logits.\"\"\"\n        if grow_step !=0:\n            new_tokens_values, new_tokens_set = logits.topk(k=31)\n        else:\n            new_tokens_values, new_tokens_set = logits.topk(k=127)\n        i=0\n        if grow_step !=0:\n            parent_nodes = parent_nodes[1:]\n        for parent_node in parent_nodes:\n            new_token_value = new_tokens_values[i]\n            new_token_set = new_tokens_set[i]\n            if parent_node.updated == False:\n                for value, logit in zip(new_token_set, new_token_value):\n                    child = TreeNode(logit=logit.item(), value=value.item(), parent=parent_node, depth=parent_node.depth + 1, updated=False)\n                    parent_node.add_child(child)\n                parent_node.updated = True\n            i+=1\n        prune_tree(self.root, keep=127)  # Assuming root is defined and accessible\n    \n    @torch.inference_mode()\n    def collective_grow_dynamic(self, benchmark=False, grow_step = None):\n        \n        if benchmark:\n            x1 = 0.0\n            x2 = 0.0\n        \n        if benchmark:\n                torch.cuda.synchronize()\n                t1 = time.time()\n        if grow_step == 0:\n           sampling_logits = self.draft_logits[0].unsqueeze(0)\n        else:\n           sampling_logits = self.draft_logits[1:128]\n        nodes = [node for node in get_all_nodes(self.root)]\n        self.update_tree_with_logits(sampling_logits, nodes, grow_step)\n        self.tokens[self.num_nodes: self.num_nodes + 127] = torch.tensor(get_all_nodes_value(self.root)[1:])\n        if benchmark:\n                    torch.cuda.synchronize()\n                    t2 = time.time()\n                    x1 += (t2 - t1)\n        # self.num_nodes = self.num_nodes + 128\n        # nodes = [node for node in get_all_nodes(self.root)]\n        \n        position_ids = torch.tensor(get_all_nodes_depth(self.root)[1:])+self.num_nodes-1\n        attn_mask = self.attn_mask[self.num_nodes: self.num_nodes+127]\n        tree_mask = generate_mask_for_pruned_tree(self.root)\n        tree_mask = (tree_mask == 0).type(self.dtype)\n        tree_mask.masked_fill_(tree_mask > 0, torch.finfo(self.dtype).min)\n        attn_mask[0:127 , self.num_nodes:self.num_nodes+127] = tree_mask\n        attn_mask = attn_mask[None, None, :, :]\n\n        draft_model_outputs = self.draft_model_engine.inference(\n            input_ids = self.tokens[self.num_nodes: self.num_nodes+127].unsqueeze(0),\n            position_ids = position_ids.unsqueeze(0),\n            attn_mask = attn_mask,\n            storage_ids=self.storage_ids[self.num_nodes: self.num_nodes+127]\n            \n        )\n        self.draft_kv_len = self.num_nodes+127\n        self.draft_logits[1:128] = draft_model_outputs[0][-127:]\n        if benchmark:\n                    torch.cuda.synchronize()\n                    t3 = time.time()\n                    x2 += (t3 - t2)\n        if benchmark:\n            return x1, x2\n        return None\n    @torch.inference_mode()\n    def accept_step(self, parent_id :int) ->ChildrenAccept:\n        logits_id = parent_id - (self.ground_truth_len - 1)\n        \n        target_token = self.target_token[logits_id]\n        children = self.Successors[logits_id]\n        if len(children) == 0:\n            return -1\n        \n        for pos in children:\n\n            token = self.tokens[pos + (self.ground_truth_len - 1)]\n            if token == target_token:\n                return pos + (self.ground_truth_len - 1)\n        \n        return -1\n\n\n        \n    @torch.inference_mode()\n    def verify(self, benchmark = False):\n        new_node_num = (self.num_nodes - self.ground_truth_len + 1)\n        if self.target_kv_len == 0:\n            start_pos = 0\n            end_pos = self.num_nodes\n            attn_mask = self.attn_mask[start_pos: end_pos, :end_pos]\n            attn_mask = attn_mask[None, None, :, :]\n            if benchmark:\n                torch.cuda.synchronize()\n                t1 = time.time()\n            target_model_outputs = self.target_model_engine.inference(input_ids = self.tokens[start_pos : end_pos].unsqueeze(0), \n                                    position_ids = self.position_ids[start_pos : end_pos].unsqueeze(0), attn_mask = attn_mask, \n                                    storage_ids=self.storage_ids[start_pos : end_pos])\n            if benchmark:\n                torch.cuda.synchronize()\n                t2 = time.time()\n            self.target_logits :torch.FloatTensor= target_model_outputs[0][self.ground_truth_len - 1:]\n\n        else:\n            start_pos = self.target_kv_len\n            end_pos = self.num_nodes\n            attn_mask = self.attn_mask[start_pos: end_pos, :end_pos]\n            attn_mask = attn_mask[None, None, :, :]\n            if benchmark:\n                torch.cuda.synchronize()\n                t1 = time.time()\n            target_model_outputs = self.target_model_engine.inference(input_ids = self.tokens[start_pos : end_pos].unsqueeze(0), \n                                        position_ids =self.position_ids[start_pos : end_pos].unsqueeze(0), attn_mask = attn_mask,\n                                        storage_ids=self.storage_ids[start_pos : end_pos])\n            if benchmark:\n                torch.cuda.synchronize()\n                t2 = time.time()\n            self.target_logits :torch.FloatTensor = target_model_outputs[0][-(new_node_num):]\n        \n        self.target_logits = get_sampling_logits(logits=self.target_logits, top_p=self.top_p, T=self.temperature, replicate=False)\n        self.target_logits = softmax(self.target_logits / self.temperature, dim=-1)\n        #self.target_token = self.target_logits.argmax(dim=-1)\n        self.target_token = self.target_logits.multinomial(num_samples=1)\n        accept_list = self.seq_to_use[:self.ground_truth_len]\n        \n        terminal = False\n        while True:\n            parent_id = accept_list[-1]\n            pos = self.accept_step(parent_id=parent_id)\n            if pos != -1:\n                accept_list.append(pos)\n                if self.tokens[pos] == 0 or self.tokens[pos] == 2:\n                     terminal = True\n                     break\n            else:\n                break\n        if benchmark:\n            torch.cuda.synchronize()\n            t3 = time.time()\n        accept_length = len(accept_list)\n        self.tokens[:accept_length] = self.tokens[accept_list]\n        if not terminal:\n            self.tokens[accept_length] = self.target_token[accept_list[-1] - self.ground_truth_len + 1].reshape(1)\n            self.draft_model_engine.engine.kv_cache.gather_kv_incremental(accept_list[self.ground_truth_len:], self.ground_truth_len)\n            self.target_model_engine.engine.kv_cache.gather_kv_incremental(accept_list[self.ground_truth_len:], self.ground_truth_len)\n            if benchmark:\n                torch.cuda.synchronize()\n                t4 = time.time()\n                self.prepare_for_next_iter(accept_list, self.tokens[:accept_length+1])\n                return self.tokens[:accept_length+1], accept_length, accept_length, t2 - t1, t3-t2, t4 - t3, terminal\n            self.prepare_for_next_iter(accept_list, self.tokens[:accept_length+1])\n            return self.tokens[:accept_length+1], accept_length, accept_length, terminal\n            \n        else:\n             if benchmark:\n                torch.cuda.synchronize()\n                t4 = time.time()\n                return self.tokens[:accept_length], accept_length, accept_length, t2 - t1, t3-t2, t4 - t3, terminal\n             return self.tokens[:accept_length], accept_length, accept_length, terminal\n    def verbose(self):\n        super().verbose()\n    def construct_dynamic_tree(self, benchmark = False):\n        if benchmark:\n            sample_time = 0\n            compute_time = 0\n        for i in range(self.draft_step - 1):\n                if benchmark:\n                        _, t1, t2 = self.collective_grow_static(self.grow_map_roots_gpu[i], self.grow_map['branches'][i], benchmark=benchmark, grow_step=i)\n                        sample_time += t1\n                        compute_time += t2   \n                else:\n                        self.collective_grow_dynamic(grow_step=i)\n        self.num_nodes = self.num_nodes+127\n        self.Successors = generate_successors_list(self.root)\n        if benchmark:\n            return sample_time, compute_time\n        else:\n            return None\n    def prepare_for_next_iter(self, accept_list: list[int], valid_tokens :torch.LongTensor):\n        if len(accept_list) + 1 > self.max_target_seq:\n              return \n        # self.tokens[:len(valid_tokens)] = valid_tokens\n        self.position_ids[:len(accept_list)] =  self.position_ids[accept_list]\n        self.position_ids[len(accept_list)] = len(accept_list) \n        self.position_ids[len(valid_tokens) : len(valid_tokens) + self.tree_size - 1] = (self.depth + len(valid_tokens) - 1)\n        self.ground_truth_len = len(valid_tokens)\n        self.num_nodes = len(valid_tokens)\n\n        total_nodes = len(valid_tokens) + self.tree_size - 1\n        # self.attn_mask.fill_(torch.finfo(self.dtype).min)\n        # self.attn_mask[:self.num_nodes, :self.num_nodes] = _make_causal_mask((1, self.num_nodes),dtype=self.dtype, device=self.device)\n        # self.attn_mask[len(valid_tokens) : len(valid_tokens) + self.tree_size - 1, : len(valid_tokens)] = 0.0\n        # self.attn_mask[len(valid_tokens) : len(valid_tokens) + self.tree_size - 1, len(valid_tokens) : len(valid_tokens) + self.tree_size - 1] = self.tree_mask[1:, 1:]\n\n        self.attn_mask = self.full_attn_mask[self.max_length - total_nodes: 2 * self.max_length - total_nodes, self.max_length - total_nodes: 2 * self.max_length - total_nodes]\n\n        \n        draft_model_outputs = self.draft_model_engine.graph_inference(input_ids = self.tokens[len(accept_list): self.num_nodes].unsqueeze(0), \n                                                    storage_ids=self.storage_ids[len(accept_list): self.num_nodes],\n                                                    position_ids=self.position_ids[len(accept_list): self.num_nodes].unsqueeze(0),\n                                                    attn_mask=self.attn_mask[len(accept_list): self.num_nodes][None, None, :, :])\n        \n        self.draft_logits[0] :torch.FloatTensor = draft_model_outputs[...,-1,:][0]\n\n        self.draft_kv_len = self.num_nodes\n        self.target_kv_len = len(accept_list)\n        self.root = TreeNode()\n        \n\n\n        \n\n\n"
        }
    ]
}